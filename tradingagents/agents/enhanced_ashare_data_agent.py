# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆAè‚¡æ•°æ®æ™ºèƒ½ä½“
é›†æˆå¤šæ•°æ®æºé‡‡é›†ç³»ç»Ÿåˆ°ç°æœ‰Aè‚¡äº¤æ˜“æ¡†æ¶
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any, Annotated
import json

from langchain_core.language_models import BaseLanguageModel
from langchain.tools import Tool

from .data_collector_agent import DataCollectorAgent, DataCollectionTask, DataSourceType
from ..dataflows.data_source_crawlers import CrawlerFactory
from ..dataflows.data_storage_manager import DataStorageManager
from ..ashare_config import get_ashare_config

logger = logging.getLogger(__name__)

class EnhancedAShareDataAgent:
    """å¢å¼ºç‰ˆAè‚¡æ•°æ®æ™ºèƒ½ä½“
    
    é›†æˆäº†å¼ºå¤§çš„æ•°æ®é‡‡é›†åŠŸèƒ½ï¼Œæ”¯æŒï¼š
    - å·¨æ½®ä¿¡æ¯ç½‘
    - æ–°æµªè´¢ç»
    - ä¸œæ–¹è´¢å¯Œç½‘
    - ä¸­å›½è¯åˆ¸ç½‘
    - æ·±äº¤æ‰€äº’åŠ¨æ˜“
    - ç¬¬ä¸€è´¢ç»
    - éŸ­ç ”å…¬ç¤¾
    """
    
    def __init__(self, llm: BaseLanguageModel):
        self.llm = llm
        self.config = get_ashare_config()
        
        # åˆå§‹åŒ–æ•°æ®é‡‡é›†å™¨å’Œå­˜å‚¨ç®¡ç†å™¨
        self.data_collector = DataCollectorAgent(llm)
        self.storage_manager = DataStorageManager()
        
        # æ•°æ®æºæ˜ å°„
        self.data_source_map = {
            "å·¨æ½®ä¿¡æ¯ç½‘": DataSourceType.JUCHAO,
            "æ–°æµªè´¢ç»": DataSourceType.SINA,
            "ä¸œæ–¹è´¢å¯Œç½‘": DataSourceType.EASTMONEY,
            "ä¸­å›½è¯åˆ¸ç½‘": DataSourceType.CNS,
            "æ·±äº¤æ‰€äº’åŠ¨æ˜“": DataSourceType.SZSE_INTERACT,
            "ç¬¬ä¸€è´¢ç»": DataSourceType.YICAI,
            "éŸ­ç ”å…¬ç¤¾": DataSourceType.JIUYAN
        }
        
        logger.info("å¢å¼ºç‰ˆAè‚¡æ•°æ®æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    def create_langchain_tools(self) -> List[Tool]:
        """åˆ›å»ºLangChainå·¥å…·é›†"""
        tools = [
            Tool(
                name="get_comprehensive_stock_news",
                description="è·å–Aè‚¡è‚¡ç¥¨çš„å…¨é¢æ–°é—»æ•°æ®ï¼Œæ•´åˆå¤šä¸ªæ•°æ®æº",
                func=self.get_comprehensive_stock_news
            ),
            Tool(
                name="get_stock_announcements",
                description="è·å–Aè‚¡è‚¡ç¥¨çš„å…¬å‘Šä¿¡æ¯ï¼Œæ¥æºäºå·¨æ½®ä¿¡æ¯ç½‘ç­‰å®˜æ–¹æ¸ é“",
                func=self.get_stock_announcements
            ),
            Tool(
                name="get_interactive_qa",
                description="è·å–æ·±äº¤æ‰€äº’åŠ¨æ˜“çš„æŠ•èµ„è€…é—®ç­”æ•°æ®",
                func=self.get_interactive_qa
            ),
            Tool(
                name="get_market_sentiment_analysis",
                description="è·å–å¸‚åœºæƒ…ç»ªåˆ†æï¼Œæ•´åˆå„å¤§è´¢ç»åª’ä½“è§‚ç‚¹",
                func=self.get_market_sentiment_analysis
            ),
            Tool(
                name="get_industry_analysis",
                description="è·å–è¡Œä¸šåˆ†ææŠ¥å‘Šå’Œç›¸å…³æ–°é—»",
                func=self.get_industry_analysis
            ),
            Tool(
                name="search_stock_data",
                description="æœç´¢è‚¡ç¥¨ç›¸å…³çš„æ‰€æœ‰ç±»å‹æ•°æ®ï¼ˆæ–°é—»ã€å…¬å‘Šã€äº’åŠ¨ç­‰ï¼‰",
                func=self.search_stock_data
            ),
            Tool(
                name="get_data_quality_report",
                description="è·å–æ•°æ®è´¨é‡æŠ¥å‘Šå’Œç»Ÿè®¡ä¿¡æ¯",
                func=self.get_data_quality_report
            )
        ]
        
        return tools
    
    def get_comprehensive_stock_news(
        self,
        stock_code: Annotated[str, "Aè‚¡è‚¡ç¥¨ä»£ç ï¼Œå¦‚'000001'æˆ–'600036'"],
        days_back: Annotated[int, "å›çœ‹å¤©æ•°ï¼Œé»˜è®¤7å¤©"] = 7
    ) -> str:
        """è·å–ç»¼åˆè‚¡ç¥¨æ–°é—»"""
        try:
            logger.info(f"å¼€å§‹è·å– {stock_code} çš„ç»¼åˆæ–°é—»æ•°æ®")
            
            # åˆ›å»ºå¤šä¸ªé‡‡é›†ä»»åŠ¡
            tasks = []
            for source_name, source_type in self.data_source_map.items():
                if source_type in [DataSourceType.SINA, DataSourceType.EASTMONEY, 
                                 DataSourceType.CNS, DataSourceType.YICAI]:
                    task = DataCollectionTask(
                        source_type=source_type,
                        target=stock_code,
                        data_type="news",
                        priority=1 if source_type == DataSourceType.SINA else 2
                    )
                    tasks.append(task)
            
            # æ·»åŠ ä»»åŠ¡åˆ°é‡‡é›†é˜Ÿåˆ—
            for task in tasks:
                self.data_collector.add_collection_task(task)
            
            # è¿è¡Œé‡‡é›†
            asyncio.run(self.data_collector.run_collection_pipeline())
            
            # ä»æ•°æ®åº“è·å–ç»“æœ
            search_results = self.storage_manager.search_data(
                keyword=stock_code,
                data_types=['news'],
                limit=50
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            news_data = search_results.get('news', [])
            if not news_data:
                return f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„æ–°é—»æ•°æ®"
            
            # æŒ‰æ¥æºå’Œæ—¶é—´æ•´ç†
            report = f"## {stock_code} ç»¼åˆæ–°é—»æ±‡æ€»ï¼ˆè¿‘{days_back}å¤©ï¼‰\n\n"
            
            # æŒ‰æ¥æºåˆ†ç»„
            source_groups = {}
            for news in news_data:
                source = news['source']
                if source not in source_groups:
                    source_groups[source] = []
                source_groups[source].append(news)
            
            for source, news_list in source_groups.items():
                report += f"### {source} ({len(news_list)}æ¡)\n\n"
                for i, news in enumerate(news_list[:5], 1):  # æ¯ä¸ªæ¥æºæœ€å¤š5æ¡
                    report += f"**{i}. {news['title']}**\n"
                    report += f"æ—¶é—´: {news['publish_time']} | è´¨é‡: {news['quality']}\n"
                    if news.get('url'):
                        report += f"é“¾æ¥: {news['url']}\n"
                    report += "\n"
                report += "---\n\n"
            
            # æ·»åŠ æ•°æ®è´¨é‡ç»Ÿè®¡
            total_news = len(news_data)
            high_quality = len([n for n in news_data if n['quality'] in ['excellent', 'good']])
            report += f"### æ•°æ®è´¨é‡ç»Ÿè®¡\n"
            report += f"- æ€»æ–°é—»æ•°: {total_news}\n"
            report += f"- é«˜è´¨é‡æ–°é—»: {high_quality} ({high_quality/total_news*100:.1f}%)\n"
            report += f"- æ•°æ®æºè¦†ç›–: {len(source_groups)}ä¸ª\n"
            
            return report
            
        except Exception as e:
            logger.error(f"è·å–ç»¼åˆæ–°é—»å¤±è´¥: {str(e)}")
            return f"è·å–è‚¡ç¥¨ {stock_code} æ–°é—»æ•°æ®å¤±è´¥: {str(e)}"
    
    def get_stock_announcements(
        self,
        stock_code: Annotated[str, "Aè‚¡è‚¡ç¥¨ä»£ç "],
        days_back: Annotated[int, "å›çœ‹å¤©æ•°"] = 30
    ) -> str:
        """è·å–è‚¡ç¥¨å…¬å‘Š"""
        try:
            logger.info(f"å¼€å§‹è·å– {stock_code} çš„å…¬å‘Šæ•°æ®")
            
            # åˆ›å»ºå…¬å‘Šé‡‡é›†ä»»åŠ¡
            task = DataCollectionTask(
                source_type=DataSourceType.JUCHAO,
                target=stock_code,
                data_type="announcement",
                priority=1
            )
            
            self.data_collector.add_collection_task(task)
            asyncio.run(self.data_collector.run_collection_pipeline())
            
            # è·å–ç»“æœ
            search_results = self.storage_manager.search_data(
                keyword=stock_code,
                data_types=['announcement'],
                limit=30
            )
            
            announcements = search_results.get('announcement', [])
            if not announcements:
                return f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„å…¬å‘Šæ•°æ®"
            
            report = f"## {stock_code} å…¬å‘Šæ±‡æ€»ï¼ˆè¿‘{days_back}å¤©ï¼‰\n\n"
            
            for i, ann in enumerate(announcements, 1):
                report += f"### {i}. {ann['title']}\n"
                report += f"**ç±»å‹**: {ann.get('type', 'æœªçŸ¥')} | **å‘å¸ƒæ—¶é—´**: {ann['publish_time']}\n"
                report += f"**æ¥æº**: {ann['source']} | **è´¨é‡**: {ann['quality']}\n"
                if ann.get('url'):
                    report += f"**é“¾æ¥**: {ann['url']}\n"
                report += "\n---\n\n"
            
            return report
            
        except Exception as e:
            logger.error(f"è·å–å…¬å‘Šå¤±è´¥: {str(e)}")
            return f"è·å–è‚¡ç¥¨ {stock_code} å…¬å‘Šæ•°æ®å¤±è´¥: {str(e)}"
    
    def get_interactive_qa(
        self,
        stock_code: Annotated[str, "Aè‚¡è‚¡ç¥¨ä»£ç "],
        days_back: Annotated[int, "å›çœ‹å¤©æ•°"] = 30
    ) -> str:
        """è·å–äº’åŠ¨é—®ç­”"""
        try:
            logger.info(f"å¼€å§‹è·å– {stock_code} çš„äº’åŠ¨é—®ç­”æ•°æ®")
            
            # åˆ›å»ºäº’åŠ¨æ˜“é‡‡é›†ä»»åŠ¡
            task = DataCollectionTask(
                source_type=DataSourceType.SZSE_INTERACT,
                target=stock_code,
                data_type="interaction",
                priority=1
            )
            
            self.data_collector.add_collection_task(task)
            asyncio.run(self.data_collector.run_collection_pipeline())
            
            # è·å–ç»“æœ
            search_results = self.storage_manager.search_data(
                keyword=stock_code,
                data_types=['interaction'],
                limit=20
            )
            
            interactions = search_results.get('interaction', [])
            if not interactions:
                return f"æœªæ‰¾åˆ°è‚¡ç¥¨ {stock_code} çš„äº’åŠ¨é—®ç­”æ•°æ®"
            
            report = f"## {stock_code} æŠ•èµ„è€…äº’åŠ¨é—®ç­”ï¼ˆè¿‘{days_back}å¤©ï¼‰\n\n"
            
            for i, qa in enumerate(interactions, 1):
                report += f"### é—®ç­” {i}\n"
                report += f"**é—®é¢˜æ—¶é—´**: {qa['question_time']}\n"
                report += f"**é—®é¢˜**: {qa['question']}\n\n"
                report += f"**å›ç­”**: {qa['answer']}\n"
                report += f"**æ¥æº**: {qa['source']} | **è´¨é‡**: {qa['quality']}\n"
                report += "\n---\n\n"
            
            return report
            
        except Exception as e:
            logger.error(f"è·å–äº’åŠ¨é—®ç­”å¤±è´¥: {str(e)}")
            return f"è·å–è‚¡ç¥¨ {stock_code} äº’åŠ¨é—®ç­”å¤±è´¥: {str(e)}"
    
    def get_market_sentiment_analysis(
        self,
        keyword: Annotated[str, "æœç´¢å…³é”®è¯ï¼Œå¦‚è‚¡ç¥¨ä»£ç ã€è¡Œä¸šåç§°ç­‰"],
        days_back: Annotated[int, "å›çœ‹å¤©æ•°"] = 7
    ) -> str:
        """è·å–å¸‚åœºæƒ…ç»ªåˆ†æ"""
        try:
            logger.info(f"å¼€å§‹è·å– {keyword} çš„å¸‚åœºæƒ…ç»ªåˆ†æ")
            
            # ä»å¤šä¸ªæ•°æ®æºè·å–ç›¸å…³æ•°æ®
            all_data = self.storage_manager.search_data(
                keyword=keyword,
                data_types=['news', 'announcement', 'interaction'],
                limit=100
            )
            
            if not any(all_data.values()):
                return f"æœªæ‰¾åˆ°å…³äº {keyword} çš„ç›¸å…³æ•°æ®è¿›è¡Œæƒ…ç»ªåˆ†æ"
            
            report = f"## {keyword} å¸‚åœºæƒ…ç»ªåˆ†ææŠ¥å‘Š\n\n"
            
            # ç»Ÿè®¡å„ç±»æ•°æ®æ•°é‡
            news_count = len(all_data.get('news', []))
            ann_count = len(all_data.get('announcement', []))
            qa_count = len(all_data.get('interaction', []))
            
            report += f"### æ•°æ®æ¦‚è§ˆ\n"
            report += f"- æ–°é—»æ•°æ®: {news_count}æ¡\n"
            report += f"- å…¬å‘Šæ•°æ®: {ann_count}æ¡\n"
            report += f"- äº’åŠ¨é—®ç­”: {qa_count}æ¡\n"
            report += f"- æ•°æ®æ—¶é—´èŒƒå›´: è¿‘{days_back}å¤©\n\n"
            
            # æ•°æ®æºåˆ†å¸ƒ
            all_sources = set()
            for data_type in all_data.values():
                for item in data_type:
                    all_sources.add(item.get('source', ''))
            
            report += f"### æ•°æ®æºè¦†ç›–\n"
            report += f"è¦†ç›–æ•°æ®æº: {', '.join(sorted(all_sources))}\n\n"
            
            # è´¨é‡åˆ†æ
            all_items = []
            for data_type in all_data.values():
                all_items.extend(data_type)
            
            quality_dist = {}
            for item in all_items:
                quality = item.get('quality', 'unknown')
                quality_dist[quality] = quality_dist.get(quality, 0) + 1
            
            report += f"### æ•°æ®è´¨é‡åˆ†å¸ƒ\n"
            for quality, count in sorted(quality_dist.items()):
                percentage = count / len(all_items) * 100
                report += f"- {quality}: {count}æ¡ ({percentage:.1f}%)\n"
            
            # æ—¶é—´è¶‹åŠ¿åˆ†æ
            report += f"\n### çƒ­åº¦è¶‹åŠ¿\n"
            report += f"åŸºäºè¿‘{days_back}å¤©çš„æ•°æ®ï¼Œ{keyword}ç›¸å…³ä¿¡æ¯æ€»è®¡{len(all_items)}æ¡ï¼Œ"
            
            if len(all_items) > 50:
                report += "ä¿¡æ¯é‡è¾ƒå¤§ï¼Œå¸‚åœºå…³æ³¨åº¦è¾ƒé«˜ã€‚\n"
            elif len(all_items) > 20:
                report += "ä¿¡æ¯é‡é€‚ä¸­ï¼Œæœ‰ä¸€å®šå¸‚åœºå…³æ³¨ã€‚\n"
            else:
                report += "ä¿¡æ¯é‡è¾ƒå°‘ï¼Œå¸‚åœºå…³æ³¨åº¦ä¸€èˆ¬ã€‚\n"
            
            return report
            
        except Exception as e:
            logger.error(f"å¸‚åœºæƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}")
            return f"è·å– {keyword} å¸‚åœºæƒ…ç»ªåˆ†æå¤±è´¥: {str(e)}"
    
    def get_industry_analysis(
        self,
        industry: Annotated[str, "è¡Œä¸šåç§°ï¼Œå¦‚'é“¶è¡Œ'ã€'ç§‘æŠ€'ã€'åŒ»è¯'ç­‰"],
        days_back: Annotated[int, "å›çœ‹å¤©æ•°"] = 14
    ) -> str:
        """è·å–è¡Œä¸šåˆ†æ"""
        try:
            logger.info(f"å¼€å§‹è·å– {industry} è¡Œä¸šåˆ†æ")
            
            # åˆ›å»ºè¡Œä¸šæ–°é—»é‡‡é›†ä»»åŠ¡
            tasks = [
                DataCollectionTask(
                    source_type=DataSourceType.YICAI,
                    target=industry,
                    data_type="news",
                    priority=1
                ),
                DataCollectionTask(
                    source_type=DataSourceType.CNS,
                    target=f"{industry}è¡Œä¸š",
                    data_type="policy",
                    priority=1
                )
            ]
            
            for task in tasks:
                self.data_collector.add_collection_task(task)
            
            asyncio.run(self.data_collector.run_collection_pipeline())
            
            # æœç´¢ç›¸å…³æ•°æ®
            search_results = self.storage_manager.search_data(
                keyword=industry,
                data_types=['news', 'announcement'],
                limit=30
            )
            
            news_data = search_results.get('news', [])
            ann_data = search_results.get('announcement', [])
            
            if not news_data and not ann_data:
                return f"æœªæ‰¾åˆ° {industry} è¡Œä¸šçš„ç›¸å…³åˆ†ææ•°æ®"
            
            report = f"## {industry}è¡Œä¸šåˆ†ææŠ¥å‘Šï¼ˆè¿‘{days_back}å¤©ï¼‰\n\n"
            
            if news_data:
                report += f"### è¡Œä¸šæ–°é—»åŠ¨æ€ ({len(news_data)}æ¡)\n\n"
                for i, news in enumerate(news_data[:10], 1):
                    report += f"**{i}. {news['title']}**\n"
                    report += f"æ¥æº: {news['source']} | æ—¶é—´: {news['publish_time']}\n\n"
                report += "---\n\n"
            
            if ann_data:
                report += f"### ç›¸å…³å…¬å‘Šä¿¡æ¯ ({len(ann_data)}æ¡)\n\n"
                for i, ann in enumerate(ann_data[:5], 1):
                    report += f"**{i}. {ann['title']}**\n"
                    report += f"ç±»å‹: {ann.get('type', 'æœªçŸ¥')} | æ—¶é—´: {ann['publish_time']}\n\n"
                report += "---\n\n"
            
            # è¡Œä¸šå…³æ³¨åº¦åˆ†æ
            total_items = len(news_data) + len(ann_data)
            report += f"### è¡Œä¸šå…³æ³¨åº¦è¯„ä¼°\n"
            report += f"- æ€»ä¿¡æ¯é‡: {total_items}æ¡\n"
            
            if total_items > 30:
                report += "- å…³æ³¨åº¦: ğŸ”´ é«˜çƒ­åº¦è¡Œä¸š\n"
            elif total_items > 15:
                report += "- å…³æ³¨åº¦: ğŸŸ¡ ä¸­ç­‰çƒ­åº¦è¡Œä¸š\n"
            else:
                report += "- å…³æ³¨åº¦: ğŸŸ¢ å¸¸è§„å…³æ³¨è¡Œä¸š\n"
            
            return report
            
        except Exception as e:
            logger.error(f"è¡Œä¸šåˆ†æå¤±è´¥: {str(e)}")
            return f"è·å– {industry} è¡Œä¸šåˆ†æå¤±è´¥: {str(e)}"
    
    def search_stock_data(
        self,
        query: Annotated[str, "æœç´¢æŸ¥è¯¢ï¼Œå¯ä»¥æ˜¯è‚¡ç¥¨ä»£ç ã€å…¬å¸åç§°æˆ–å…³é”®è¯"],
        data_types: Annotated[str, "æ•°æ®ç±»å‹ï¼Œç”¨é€—å·åˆ†éš”ï¼šnews,announcement,interaction"] = "news,announcement,interaction"
    ) -> str:
        """æœç´¢è‚¡ç¥¨æ•°æ®"""
        try:
            # è§£ææ•°æ®ç±»å‹
            types_list = [t.strip() for t in data_types.split(',')]
            
            # æ‰§è¡Œæœç´¢
            results = self.storage_manager.search_data(
                keyword=query,
                data_types=types_list,
                limit=50
            )
            
            if not any(results.values()):
                return f"æœªæ‰¾åˆ°å…³äº '{query}' çš„ç›¸å…³æ•°æ®"
            
            report = f"## '{query}' æœç´¢ç»“æœ\n\n"
            
            for data_type, items in results.items():
                if not items:
                    continue
                
                type_name = {
                    'news': 'æ–°é—»',
                    'announcement': 'å…¬å‘Š', 
                    'interaction': 'äº’åŠ¨é—®ç­”'
                }.get(data_type, data_type)
                
                report += f"### {type_name} ({len(items)}æ¡)\n\n"
                
                for i, item in enumerate(items[:10], 1):
                    if data_type == 'news':
                        report += f"**{i}. {item['title']}**\n"
                        report += f"æ¥æº: {item['source']} | æ—¶é—´: {item['publish_time']} | è´¨é‡: {item['quality']}\n"
                    elif data_type == 'announcement':
                        report += f"**{i}. {item['title']}**\n"
                        report += f"ç±»å‹: {item.get('type', 'æœªçŸ¥')} | æ—¶é—´: {item['publish_time']} | è´¨é‡: {item['quality']}\n"
                    elif data_type == 'interaction':
                        report += f"**{i}. é—®ç­”**\n"
                        report += f"é—®é¢˜: {item['question']}\n"
                        report += f"æ—¶é—´: {item['question_time']} | è´¨é‡: {item['quality']}\n"
                    
                    if item.get('url'):
                        report += f"é“¾æ¥: {item['url']}\n"
                    report += "\n"
                
                report += "---\n\n"
            
            return report
            
        except Exception as e:
            logger.error(f"æœç´¢æ•°æ®å¤±è´¥: {str(e)}")
            return f"æœç´¢ '{query}' å¤±è´¥: {str(e)}"
    
    def get_data_quality_report(self) -> str:
        """è·å–æ•°æ®è´¨é‡æŠ¥å‘Š"""  
        try:
            stats = self.storage_manager.get_data_statistics()
            
            report = "## Aè‚¡æ•°æ®åº“è´¨é‡æŠ¥å‘Š\n\n"
            
            # æ•°æ®æ€»é‡ç»Ÿè®¡
            report += "### æ•°æ®æ€»é‡ç»Ÿè®¡\n"
            report += f"- æ–°é—»æ•°æ®: {stats['news_count']:,}æ¡\n"
            report += f"- å…¬å‘Šæ•°æ®: {stats['announcement_count']:,}æ¡\n"
            report += f"- äº’åŠ¨é—®ç­”: {stats['interaction_count']:,}æ¡\n"
            report += f"- ç ”ç©¶æŠ¥å‘Š: {stats['research_count']:,}æ¡\n"
            total = stats['news_count'] + stats['announcement_count'] + stats['interaction_count'] + stats['research_count']
            report += f"- **æ€»æ•°æ®é‡**: {total:,}æ¡\n\n"
            
            # è´¨é‡åˆ†å¸ƒç»Ÿè®¡
            report += "### æ•°æ®è´¨é‡åˆ†å¸ƒ\n"
            quality_dist = stats['quality_distribution']
            
            for quality, counts in quality_dist.items():
                total_quality = counts['total']
                if total_quality > 0:
                    percentage = total_quality / total * 100
                    quality_name = {
                        'excellent': 'ä¼˜ç§€',
                        'good': 'è‰¯å¥½',
                        'average': 'ä¸€èˆ¬',
                        'poor': 'è¾ƒå·®',
                        'invalid': 'æ— æ•ˆ'
                    }.get(quality, quality)
                    
                    report += f"**{quality_name}**: {total_quality:,}æ¡ ({percentage:.1f}%)\n"
                    report += f"  - æ–°é—»: {counts['news']:,}æ¡\n"
                    report += f"  - å…¬å‘Š: {counts['announcement']:,}æ¡\n"
                    report += f"  - äº’åŠ¨: {counts['interaction']:,}æ¡\n\n"
            
            # è¿‘æœŸæ•°æ®ç»Ÿè®¡
            report += "### è¿‘7å¤©æ•°æ®å¢é•¿\n"
            recent = stats['recent_data']
            report += f"- æ–°å¢æ–°é—»: {recent['news_count']:,}æ¡\n"
            report += f"- æ–°å¢å…¬å‘Š: {recent['announcement_count']:,}æ¡\n"
            report += f"- æ–°å¢äº’åŠ¨: {recent['interaction_count']:,}æ¡\n"
            
            recent_total = recent['news_count'] + recent['announcement_count'] + recent['interaction_count']
            if total > 0:
                growth_rate = recent_total / total * 100
                report += f"- å¢é•¿ç‡: {growth_rate:.2f}%\n\n"
            
            # æ•°æ®è´¨é‡å»ºè®®
            excellent_rate = quality_dist.get('excellent', {}).get('total', 0) / total * 100 if total > 0 else 0
            good_rate = quality_dist.get('good', {}).get('total', 0) / total * 100 if total > 0 else 0
            
            report += "### è´¨é‡è¯„ä¼°ä¸å»ºè®®\n"
            if excellent_rate + good_rate > 70:
                report += "âœ… æ•°æ®è´¨é‡ä¼˜ç§€ï¼Œé«˜è´¨é‡æ•°æ®å æ¯”è¶…è¿‡70%\n"
            elif excellent_rate + good_rate > 50:
                report += "âš ï¸ æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–æ•°æ®æº\n"
            else:
                report += "âŒ æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®é‡‡é›†è§„åˆ™\n"
            
            report += f"\næ•°æ®åº“è·¯å¾„: {self.storage_manager.db_path}\n"
            report += f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
            
            return report
            
        except Exception as e:
            logger.error(f"è·å–æ•°æ®è´¨é‡æŠ¥å‘Šå¤±è´¥: {str(e)}")
            return f"è·å–æ•°æ®è´¨é‡æŠ¥å‘Šå¤±è´¥: {str(e)}"
    
    def export_data_excel(self, output_path: str = None) -> str:
        """å¯¼å‡ºæ•°æ®åˆ°Excel"""
        try:
            if not output_path:
                output_path = f"ashare_data_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
            
            success = self.storage_manager.export_data_to_excel(output_path)
            
            if success:
                return f"æ•°æ®å¯¼å‡ºæˆåŠŸ: {output_path}"
            else:
                return "æ•°æ®å¯¼å‡ºå¤±è´¥"
                
        except Exception as e:
            logger.error(f"æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}")
            return f"æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}"

def create_enhanced_ashare_data_agent(llm: BaseLanguageModel) -> EnhancedAShareDataAgent:
    """åˆ›å»ºå¢å¼ºç‰ˆAè‚¡æ•°æ®æ™ºèƒ½ä½“"""
    return EnhancedAShareDataAgent(llm)