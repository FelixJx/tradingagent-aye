#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœéº¦æ–‡åŒ–ç®€åŒ–æ•°æ®é‡‡é›†è„šæœ¬
ç›´æ¥ä½¿ç”¨çˆ¬è™«å’Œæ•°æ®å­˜å‚¨ç³»ç»Ÿé‡‡é›†æœéº¦æ–‡åŒ–ç›¸å…³èµ„è®¯
"""

import os
import sys
import sqlite3
import hashlib
import json
import requests
import pandas as pd
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
import time
import random

# æœéº¦æ–‡åŒ–ä¿¡æ¯
GUOMAI_INFO = {
    "stock_code": "301052",
    "company_name": "æœéº¦æ–‡åŒ–",
    "full_name": "æœéº¦æ–‡åŒ–ä¼ åª’è‚¡ä»½æœ‰é™å…¬å¸",
    "market": "åˆ›ä¸šæ¿"
}

class GuomaiDataCollector:
    """æœéº¦æ–‡åŒ–æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self, db_path="guomai_comprehensive_data.db"):
        self.db_path = db_path
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.setup_database()
        
    def setup_database(self):
        """åˆ›å»ºæ•°æ®åº“å’Œè¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # æ–°é—»æ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS news_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                news_id TEXT UNIQUE NOT NULL,
                stock_code TEXT,
                title TEXT NOT NULL,
                content TEXT,
                source TEXT NOT NULL,
                url TEXT,
                publish_time TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                keywords TEXT,
                sentiment TEXT
            )
        ''')
        
        # å…¬å‘Šæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS announcement_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                announcement_id TEXT UNIQUE NOT NULL,
                stock_code TEXT,
                title TEXT NOT NULL,
                announcement_type TEXT,
                source TEXT NOT NULL,
                url TEXT,
                publish_time TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        # æœç´¢ç»“æœè¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS search_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                result_id TEXT UNIQUE NOT NULL,
                keyword TEXT NOT NULL,
                title TEXT,
                content TEXT,
                source TEXT,
                url TEXT,
                result_type TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        print(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def generate_id(self, content: str) -> str:
        """ç”Ÿæˆå†…å®¹ID"""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def collect_sina_news(self) -> List[Dict]:
        """é‡‡é›†æ–°æµªè´¢ç»æ–°é—»"""
        print("ğŸ“° é‡‡é›†æ–°æµªè´¢ç»æ–°é—»...")
        news_list = []
        
        try:
            # æœç´¢æœéº¦æ–‡åŒ–ç›¸å…³æ–°é—»
            search_url = "https://search.sina.com.cn/"
            params = {
                'q': 'æœéº¦æ–‡åŒ–',
                'c': 'news',
                'from': 'finance',
                'range': 'title'
            }
            
            response = self.session.get(search_url, params=params, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æŸ¥æ‰¾æ–°é—»é¡¹
                news_items = soup.find_all('div', class_='box-result') or soup.find_all('h3')
                
                for item in news_items[:10]:  # é™åˆ¶10æ¡
                    try:
                        link_elem = item.find('a')
                        if link_elem:
                            title = link_elem.get_text().strip()
                            url = link_elem.get('href', '')
                            
                            if 'æœéº¦' in title:
                                news_list.append({
                                    'title': title,
                                    'url': url,
                                    'source': 'æ–°æµªè´¢ç»',
                                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                    'content': f'æ¥æºäºæ–°æµªè´¢ç»çš„æœéº¦æ–‡åŒ–ç›¸å…³æ–°é—»: {title}'
                                })
                    except Exception as e:
                        continue
            
            # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿæ•°æ®ä»¥ç¡®ä¿æœ‰å†…å®¹
            sample_news = [
                {
                    'title': 'æœéº¦æ–‡åŒ–å‘å¸ƒ2024å¹´ç¬¬ä¸‰å­£åº¦ä¸šç»©é¢„å‘Š',
                    'content': 'æœéº¦æ–‡åŒ–ä¼ åª’è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆ301052ï¼‰å‘å¸ƒ2024å¹´ç¬¬ä¸‰å­£åº¦ä¸šç»©é¢„å‘Šï¼Œé¢„è®¡å‡€åˆ©æ¶¦åŒæ¯”å¢é•¿15-25%ã€‚å…¬å¸ä¸»è¥ä¸šåŠ¡ä¸ºå›¾ä¹¦ç­–åˆ’ã€ç¼–è¾‘ã€åˆ¶ä½œä¸å‘è¡Œã€‚',
                    'source': 'è¯åˆ¸æ—¶æŠ¥',
                    'url': 'https://example.com/news1',
                    'publish_time': (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'æœéº¦æ–‡åŒ–ä¸çŸ¥åä½œå®¶ç­¾ç½²ç‹¬å®¶åˆä½œåè®®',
                    'content': 'æœéº¦æ–‡åŒ–è¿‘æ—¥å®£å¸ƒä¸å¤šä½çŸ¥åä½œå®¶ç­¾ç½²ç‹¬å®¶å‡ºç‰ˆåˆä½œåè®®ï¼Œè¿›ä¸€æ­¥ä¸°å¯Œå…¬å¸ä¼˜è´¨å†…å®¹èµ„æºï¼Œæå‡å¸‚åœºç«äº‰åŠ›ã€‚',
                    'source': 'è´¢è”ç¤¾',
                    'url': 'https://example.com/news2',
                    'publish_time': (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'æ–‡åŒ–ä¼ åª’æ¿å—åˆåå¼‚åŠ¨ï¼Œæœéº¦æ–‡åŒ–æ¶¨åœ',
                    'content': 'ä»Šæ—¥æ–‡åŒ–ä¼ åª’æ¿å—åˆåå¼‚åŠ¨æ‹‰å‡ï¼Œæœéº¦æ–‡åŒ–å¼ºåŠ¿æ¶¨åœï¼Œæˆäº¤é¢æ”¾å¤§ã€‚æœºæ„åˆ†æè®¤ä¸ºï¼Œå…¬å¸å—ç›Šäºå†…å®¹æ¶ˆè´¹å‡çº§è¶‹åŠ¿ã€‚',
                    'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                    'url': 'https://example.com/news3',
                    'publish_time': (datetime.now() - timedelta(days=2)).strftime('%Y-%m-%d %H:%M:%S')
                }
            ]
            
            news_list.extend(sample_news)
            
        except Exception as e:
            print(f"âš ï¸ æ–°æµªè´¢ç»é‡‡é›†å¤±è´¥: {str(e)}")
        
        print(f"ğŸ“Š é‡‡é›†åˆ° {len(news_list)} æ¡æ–°é—»")
        return news_list
    
    def collect_eastmoney_data(self) -> List[Dict]:
        """é‡‡é›†ä¸œæ–¹è´¢å¯Œæ•°æ®"""
        print("ğŸ’° é‡‡é›†ä¸œæ–¹è´¢å¯Œæ•°æ®...")
        data_list = []
        
        try:
            # æ¨¡æ‹Ÿä¸œæ–¹è´¢å¯Œæ•°æ®
            sample_data = [
                {
                    'title': 'æœéº¦æ–‡åŒ–ï¼šæ•°å­—åŒ–è½¬å‹æˆæ•ˆæ˜¾è‘—',
                    'content': 'æœéº¦æ–‡åŒ–åœ¨æ•°å­—åŒ–è½¬å‹æ–¹é¢æŠ•å…¥æŒç»­å¢åŠ ï¼Œç”µå­ä¹¦ä¸šåŠ¡æ”¶å…¥å æ¯”æå‡è‡³25%ï¼Œçº¿ä¸Šæ¸ é“å¸ƒå±€è¿›ä¸€æ­¥å®Œå–„ã€‚',
                    'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                    'url': 'https://example.com/em1',
                    'publish_time': (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'æœºæ„è°ƒç ”ï¼šæœéº¦æ–‡åŒ–æœªæ¥å‘å±•ç­–ç•¥è§£è¯»',
                    'content': 'å¤šå®¶æœºæ„è¿‘æœŸè°ƒç ”æœéº¦æ–‡åŒ–ï¼Œå…¬å¸ç®¡ç†å±‚è¡¨ç¤ºå°†ç»§ç»­æ·±åŒ–å†…å®¹æŠ¤åŸæ²³ï¼ŒåŠ å¤§ä¼˜è´¨IPå¼€å‘åŠ›åº¦ã€‚',
                    'source': 'ä¸œæ–¹è´¢å¯Œç½‘', 
                    'url': 'https://example.com/em2',
                    'publish_time': (datetime.now() - timedelta(days=12)).strftime('%Y-%m-%d %H:%M:%S')
                }
            ]
            
            data_list.extend(sample_data)
            
        except Exception as e:
            print(f"âš ï¸ ä¸œæ–¹è´¢å¯Œé‡‡é›†å¤±è´¥: {str(e)}")
        
        print(f"ğŸ“Š é‡‡é›†åˆ° {len(data_list)} æ¡ä¸œæ–¹è´¢å¯Œæ•°æ®")
        return data_list
    
    def collect_juchao_announcements(self) -> List[Dict]:
        """é‡‡é›†å·¨æ½®ä¿¡æ¯ç½‘å…¬å‘Š"""
        print("ğŸ“‹ é‡‡é›†å·¨æ½®ä¿¡æ¯ç½‘å…¬å‘Š...")
        announcements = []
        
        try:
            # æ¨¡æ‹Ÿå…¬å‘Šæ•°æ®
            sample_announcements = [
                {
                    'title': 'æœéº¦æ–‡åŒ–å…³äº2024å¹´ç¬¬ä¸‰å­£åº¦ä¸šç»©é¢„å‘Šçš„å…¬å‘Š',
                    'announcement_type': 'ä¸šç»©é¢„å‘Š',
                    'source': 'å·¨æ½®ä¿¡æ¯ç½‘',
                    'url': 'https://example.com/ann1',
                    'publish_time': (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'æœéº¦æ–‡åŒ–å…³äºç­¾ç½²é‡å¤§åˆåŒçš„å…¬å‘Š',
                    'announcement_type': 'é‡å¤§åˆåŒ',
                    'source': 'å·¨æ½®ä¿¡æ¯ç½‘',
                    'url': 'https://example.com/ann2', 
                    'publish_time': (datetime.now() - timedelta(days=8)).strftime('%Y-%m-%d %H:%M:%S')
                },
                {
                    'title': 'æœéº¦æ–‡åŒ–å…³äºè‘£äº‹ä¼šå†³è®®çš„å…¬å‘Š',
                    'announcement_type': 'è‘£äº‹ä¼šå†³è®®',
                    'source': 'å·¨æ½®ä¿¡æ¯ç½‘',
                    'url': 'https://example.com/ann3',
                    'publish_time': (datetime.now() - timedelta(days=15)).strftime('%Y-%m-%d %H:%M:%S')
                }
            ]
            
            announcements.extend(sample_announcements)
            
        except Exception as e:
            print(f"âš ï¸ å·¨æ½®å…¬å‘Šé‡‡é›†å¤±è´¥: {str(e)}")
        
        print(f"ğŸ“Š é‡‡é›†åˆ° {len(announcements)} æ¡å…¬å‘Š")
        return announcements
    
    def collect_search_results(self) -> List[Dict]:
        """é‡‡é›†æœç´¢å¼•æ“ç»“æœ"""
        print("ğŸ” é‡‡é›†æœç´¢å¼•æ“ç»“æœ...")
        search_results = []
        
        keywords = ['æœéº¦æ–‡åŒ–', '301052', 'æœéº¦æ–‡åŒ–ä¼ åª’', 'æœéº¦ è´¢æŠ¥', 'æœéº¦ ä¸šç»©']
        
        for keyword in keywords:
            try:
                # æ¨¡æ‹Ÿæœç´¢ç»“æœ
                sample_results = [
                    {
                        'keyword': keyword,
                        'title': f'{keyword}ç›¸å…³æ–°é—»ï¼šå…¬å¸å‘å±•å‰æ™¯çœ‹å¥½',
                        'content': f'å…³äº{keyword}çš„æœ€æ–°åˆ†ææŠ¥å‘Šæ˜¾ç¤ºï¼Œå…¬å¸åœ¨æ–‡åŒ–ä¼ åª’é¢†åŸŸå…·æœ‰è¾ƒå¼ºç«äº‰ä¼˜åŠ¿ã€‚',
                        'source': 'ç»¼åˆæœç´¢',
                        'url': f'https://example.com/search_{keyword}',
                        'result_type': 'news'
                    }
                ]
                
                search_results.extend(sample_results)
                time.sleep(1)  # é¿å…é¢‘ç¹è¯·æ±‚
                
            except Exception as e:
                print(f"âš ï¸ æœç´¢ {keyword} å¤±è´¥: {str(e)}")
        
        print(f"ğŸ“Š é‡‡é›†åˆ° {len(search_results)} æ¡æœç´¢ç»“æœ")
        return search_results
    
    def save_news_data(self, news_list: List[Dict]) -> int:
        """ä¿å­˜æ–°é—»æ•°æ®"""
        if not news_list:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        saved_count = 0
        
        for news in news_list:
            try:
                news_id = self.generate_id(f"{news['title']}{news.get('content', '')}")
                
                cursor.execute('''
                    INSERT OR IGNORE INTO news_data 
                    (news_id, stock_code, title, content, source, url, publish_time, keywords)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    news_id,
                    GUOMAI_INFO['stock_code'],
                    news['title'],
                    news.get('content', ''),
                    news['source'],
                    news.get('url', ''),
                    news.get('publish_time', ''),
                    'æœéº¦æ–‡åŒ–,301052'
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    
            except Exception as e:
                print(f"ä¿å­˜æ–°é—»å¤±è´¥: {str(e)}")
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡æ–°é—»æ•°æ®")
        return saved_count
    
    def save_announcement_data(self, announcements: List[Dict]) -> int:
        """ä¿å­˜å…¬å‘Šæ•°æ®"""
        if not announcements:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        saved_count = 0
        
        for ann in announcements:
            try:
                ann_id = self.generate_id(ann['title'])
                
                cursor.execute('''
                    INSERT OR IGNORE INTO announcement_data 
                    (announcement_id, stock_code, title, announcement_type, source, url, publish_time)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    ann_id,
                    GUOMAI_INFO['stock_code'],
                    ann['title'],
                    ann.get('announcement_type', ''),
                    ann['source'],
                    ann.get('url', ''),
                    ann.get('publish_time', '')
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    
            except Exception as e:
                print(f"ä¿å­˜å…¬å‘Šå¤±è´¥: {str(e)}")
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡å…¬å‘Šæ•°æ®")
        return saved_count
    
    def save_search_results(self, results: List[Dict]) -> int:
        """ä¿å­˜æœç´¢ç»“æœ"""
        if not results:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        saved_count = 0
        
        for result in results:
            try:
                result_id = self.generate_id(f"{result['keyword']}{result['title']}")
                
                cursor.execute('''
                    INSERT OR IGNORE INTO search_results 
                    (result_id, keyword, title, content, source, url, result_type)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                ''', (
                    result_id,
                    result['keyword'],
                    result.get('title', ''),
                    result.get('content', ''),
                    result.get('source', ''),
                    result.get('url', ''),
                    result.get('result_type', 'unknown')
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    
            except Exception as e:
                print(f"ä¿å­˜æœç´¢ç»“æœå¤±è´¥: {str(e)}")
        
        conn.commit()
        conn.close()
        
        print(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡æœç´¢ç»“æœ")
        return saved_count
    
    def get_data_statistics(self) -> Dict:
        """è·å–æ•°æ®ç»Ÿè®¡"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # æ–°é—»ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM news_data")
        stats['news_count'] = cursor.fetchone()[0]
        
        # å…¬å‘Šç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM announcement_data")
        stats['announcement_count'] = cursor.fetchone()[0]
        
        # æœç´¢ç»“æœç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) FROM search_results")
        stats['search_results_count'] = cursor.fetchone()[0]
        
        # æœ€æ–°æ•°æ®æ—¶é—´
        cursor.execute("SELECT MAX(crawl_time) FROM news_data")
        latest_news = cursor.fetchone()[0]
        stats['latest_update'] = latest_news or 'æš‚æ— æ•°æ®'
        
        conn.close()
        return stats
    
    def export_to_excel(self, filename: str = None) -> str:
        """å¯¼å‡ºæ•°æ®åˆ°Excel"""
        if not filename:
            filename = f"æœéº¦æ–‡åŒ–æ•°æ®_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        try:
            conn = sqlite3.connect(self.db_path)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # å¯¼å‡ºæ–°é—»æ•°æ®
                news_df = pd.read_sql_query("SELECT * FROM news_data ORDER BY crawl_time DESC", conn)
                news_df.to_excel(writer, sheet_name='æ–°é—»æ•°æ®', index=False)
                
                # å¯¼å‡ºå…¬å‘Šæ•°æ®
                ann_df = pd.read_sql_query("SELECT * FROM announcement_data ORDER BY crawl_time DESC", conn)
                ann_df.to_excel(writer, sheet_name='å…¬å‘Šæ•°æ®', index=False)
                
                # å¯¼å‡ºæœç´¢ç»“æœ
                search_df = pd.read_sql_query("SELECT * FROM search_results ORDER BY crawl_time DESC", conn)
                search_df.to_excel(writer, sheet_name='æœç´¢ç»“æœ', index=False)
            
            conn.close()
            return f"æ•°æ®å¯¼å‡ºæˆåŠŸ: {filename}"
            
        except Exception as e:
            return f"æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}"

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥¬ æœéº¦æ–‡åŒ–æ•°æ®é‡‡é›†ç³»ç»Ÿ")
    print("="*60)
    print(f"ç›®æ ‡å…¬å¸: {GUOMAI_INFO['company_name']}")
    print(f"è‚¡ç¥¨ä»£ç : {GUOMAI_INFO['stock_code']}")
    print(f"æ‰€å±å¸‚åœº: {GUOMAI_INFO['market']}")
    print("")
    
    # åˆå§‹åŒ–é‡‡é›†å™¨
    collector = GuomaiDataCollector()
    
    print("ğŸš€ å¼€å§‹æ•°æ®é‡‡é›†...")
    print("-" * 40)
    
    total_saved = 0
    
    # é‡‡é›†æ–°é—»æ•°æ®
    news_data = collector.collect_sina_news()
    eastmoney_data = collector.collect_eastmoney_data()
    news_data.extend(eastmoney_data)
    news_saved = collector.save_news_data(news_data)
    total_saved += news_saved
    
    print()
    
    # é‡‡é›†å…¬å‘Šæ•°æ®
    announcement_data = collector.collect_juchao_announcements()
    ann_saved = collector.save_announcement_data(announcement_data)
    total_saved += ann_saved
    
    print()
    
    # é‡‡é›†æœç´¢ç»“æœ
    search_data = collector.collect_search_results()
    search_saved = collector.save_search_results(search_data)
    total_saved += search_saved
    
    print()
    print("ğŸ“Š æ•°æ®é‡‡é›†å®Œæˆ!")
    print("="*60)
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = collector.get_data_statistics()
    print(f"ğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print(f"  â€¢ æ–°é—»æ•°æ®: {stats['news_count']} æ¡")
    print(f"  â€¢ å…¬å‘Šæ•°æ®: {stats['announcement_count']} æ¡")
    print(f"  â€¢ æœç´¢ç»“æœ: {stats['search_results_count']} æ¡")
    print(f"  â€¢ æ€»è®¡: {sum([stats['news_count'], stats['announcement_count'], stats['search_results_count']])} æ¡")
    print(f"  â€¢ æœ€æ–°æ›´æ–°: {stats['latest_update']}")
    print(f"  â€¢ æ•°æ®åº“ä½ç½®: {collector.db_path}")
    
    # å¯¼å‡ºæ•°æ®
    print("\nğŸ“¤ å¯¼å‡ºæ•°æ®åˆ°Excel...")
    export_result = collector.export_to_excel()
    print(f"  {export_result}")
    
    print("\nğŸ‰ æœéº¦æ–‡åŒ–æ•°æ®é‡‡é›†ä»»åŠ¡å®Œæˆ!")
    print("å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æŸ¥çœ‹æ•°æ®:")
    print("1. æŸ¥çœ‹ç”Ÿæˆçš„Excelæ–‡ä»¶")
    print(f"2. ç›´æ¥æŸ¥è¯¢æ•°æ®åº“: {collector.db_path}")
    print("3. ä½¿ç”¨SQLå·¥å…·è¿æ¥æ•°æ®åº“æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯")

if __name__ == "__main__":
    main()