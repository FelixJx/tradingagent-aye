#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘çš„æœéº¦æ–‡åŒ–æ•°æ®é‡‡é›†ç³»ç»Ÿ
- è‡ªåŠ¨æœç´¢å’Œä¸‹è½½GitHubçˆ¬è™«å·¥å…·
- å¤šé‡å¤‡ç”¨ç­–ç•¥
- æŒç»­é‡è¯•ç›´åˆ°æˆåŠŸ
- æ™ºèƒ½é”™è¯¯åˆ†æå’Œç­–ç•¥è°ƒæ•´
"""

import os
import sys
import git
import subprocess
import requests
import json
import sqlite3
import hashlib
import time
import random
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Any
import logging
import zipfile
import importlib.util
import shutil
from bs4 import BeautifulSoup
import pandas as pd

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('unstoppable_guomai.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GitHubCrawlerSearcher:
    """GitHubçˆ¬è™«å·¥å…·æœç´¢å™¨"""
    
    def __init__(self):
        self.github_api = "https://api.github.com"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/vnd.github.v3+json'
        })
        
    def search_crawlers(self, keywords: List[str], max_results: int = 50) -> List[Dict]:
        """æœç´¢GitHubä¸Šçš„çˆ¬è™«å·¥å…·"""
        all_crawlers = []
        
        search_terms = [
            "sina finance crawler python",
            "eastmoney crawler python",
            "cninfo crawler python",
            "juchao crawler python",
            "china stock news crawler",
            "ä¸­å›½è‚¡ç¥¨çˆ¬è™« python",
            "è´¢ç»æ–°é—»çˆ¬è™« python",
            "è‚¡ç¥¨æ•°æ®é‡‡é›† python",
            "Aè‚¡çˆ¬è™« python",
            "è¯åˆ¸æ–°é—»çˆ¬è™« python"
        ]
        
        for term in search_terms:
            try:
                logger.info(f"æœç´¢GitHubçˆ¬è™«: {term}")
                
                search_url = f"{self.github_api}/search/repositories"
                params = {
                    'q': f'{term} language:Python',
                    'sort': 'stars',
                    'order': 'desc',
                    'per_page': 10
                }
                
                response = self.session.get(search_url, params=params, timeout=30)
                response.raise_for_status()
                
                data = response.json()
                
                for item in data.get('items', []):
                    if item['stargazers_count'] >= 1:  # è‡³å°‘æœ‰1ä¸ªstar
                        crawler_info = {
                            'name': item['name'],
                            'full_name': item['full_name'],
                            'description': item['description'] or '',
                            'clone_url': item['clone_url'],
                            'stars': item['stargazers_count'],
                            'language': item['language'],
                            'updated_at': item['updated_at'],
                            'search_term': term,
                            'size': item['size']
                        }
                        all_crawlers.append(crawler_info)
                
                # é¿å…APIé™åˆ¶
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"æœç´¢ {term} å¤±è´¥: {str(e)}")
                continue
        
        # å»é‡å¹¶æŒ‰æ˜Ÿæ•°æ’åº
        unique_crawlers = {}
        for crawler in all_crawlers:
            key = crawler['full_name']
            if key not in unique_crawlers or crawler['stars'] > unique_crawlers[key]['stars']:
                unique_crawlers[key] = crawler
        
        sorted_crawlers = sorted(unique_crawlers.values(), key=lambda x: x['stars'], reverse=True)
        logger.info(f"æ‰¾åˆ° {len(sorted_crawlers)} ä¸ªæ½œåœ¨çˆ¬è™«å·¥å…·")
        
        return sorted_crawlers[:max_results]
    
    def download_and_setup_crawler(self, crawler_info: Dict, target_dir: Path) -> bool:
        """ä¸‹è½½å¹¶è®¾ç½®çˆ¬è™«å·¥å…·"""
        crawler_name = crawler_info['name']
        crawler_dir = target_dir / crawler_name
        
        try:
            if crawler_dir.exists():
                logger.info(f"çˆ¬è™« {crawler_name} å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½")
                return True
            
            logger.info(f"æ­£åœ¨ä¸‹è½½çˆ¬è™«: {crawler_info['full_name']}")
            
            # å°è¯•ä½¿ç”¨git clone
            try:
                git.Repo.clone_from(crawler_info['clone_url'], str(crawler_dir))
                logger.info(f"âœ… Git clone æˆåŠŸ: {crawler_name}")
            except Exception as e:
                logger.warning(f"Git clone å¤±è´¥ï¼Œå°è¯•ä¸‹è½½ZIP: {str(e)}")
                
                # å°è¯•ä¸‹è½½ZIPæ–‡ä»¶
                zip_url = f"https://github.com/{crawler_info['full_name']}/archive/refs/heads/main.zip"
                response = self.session.get(zip_url, timeout=60)
                
                if response.status_code != 200:
                    zip_url = zip_url.replace('/main.zip', '/master.zip')
                    response = self.session.get(zip_url, timeout=60)
                
                if response.status_code == 200:
                    zip_path = target_dir / f"{crawler_name}.zip"
                    with open(zip_path, 'wb') as f:
                        f.write(response.content)
                    
                    # è§£å‹
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(target_dir)
                    
                    # é‡å‘½åè§£å‹åçš„ç›®å½•
                    extracted_dirs = [d for d in target_dir.iterdir() if d.is_dir() and crawler_name in d.name]
                    if extracted_dirs:
                        extracted_dirs[0].rename(crawler_dir)
                    
                    zip_path.unlink()  # åˆ é™¤ZIPæ–‡ä»¶
                    logger.info(f"âœ… ZIPä¸‹è½½æˆåŠŸ: {crawler_name}")
                else:
                    raise Exception(f"æ— æ³•ä¸‹è½½ZIPæ–‡ä»¶: {response.status_code}")
            
            # å°è¯•å®‰è£…ä¾èµ–
            self._install_dependencies(crawler_dir)
            
            return True
            
        except Exception as e:
            logger.error(f"ä¸‹è½½çˆ¬è™« {crawler_name} å¤±è´¥: {str(e)}")
            if crawler_dir.exists():
                shutil.rmtree(crawler_dir, ignore_errors=True)
            return False
    
    def _install_dependencies(self, crawler_dir: Path):
        """å®‰è£…çˆ¬è™«ä¾èµ–"""
        try:
            requirements_files = [
                crawler_dir / 'requirements.txt',
                crawler_dir / 'requirements.pip',
                crawler_dir / 'pip-requirements.txt'
            ]
            
            for req_file in requirements_files:
                if req_file.exists():
                    logger.info(f"å®‰è£…ä¾èµ–: {req_file}")
                    result = subprocess.run([
                        sys.executable, '-m', 'pip', 'install', '-r', str(req_file)
                    ], capture_output=True, text=True, timeout=300)
                    
                    if result.returncode == 0:
                        logger.info(f"âœ… ä¾èµ–å®‰è£…æˆåŠŸ")
                        break
                    else:
                        logger.warning(f"âš ï¸ ä¾èµ–å®‰è£…å¤±è´¥: {result.stderr}")
            
            # å°è¯•å®‰è£…setup.py
            setup_py = crawler_dir / 'setup.py'
            if setup_py.exists():
                logger.info("å°è¯•é€šè¿‡setup.pyå®‰è£…")
                result = subprocess.run([
                    sys.executable, 'setup.py', 'install'
                ], cwd=str(crawler_dir), capture_output=True, text=True, timeout=300)
                
                if result.returncode == 0:
                    logger.info("âœ… setup.pyå®‰è£…æˆåŠŸ")
                    
        except Exception as e:
            logger.warning(f"âš ï¸ å®‰è£…ä¾èµ–å¤±è´¥: {str(e)}")

class UnstoppableDataCollector:
    """ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘çš„æ•°æ®é‡‡é›†å™¨"""
    
    def __init__(self):
        self.target_company = {
            "stock_code": "301052",
            "company_name": "æœéº¦æ–‡åŒ–",
            "keywords": ["æœéº¦æ–‡åŒ–", "301052", "æœéº¦ä¼ åª’", "æœéº¦", "GUOMAI"]
        }
        
        self.db_path = "unstoppable_guomai_data.db"
        self.crawlers_dir = Path("downloaded_crawlers")
        self.crawlers_dir.mkdir(exist_ok=True)
        
        self.setup_database()
        self.github_searcher = GitHubCrawlerSearcher()
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        self.retry_strategies = [
            self._strategy_direct_crawling,
            self._strategy_github_tools,
            self._strategy_api_endpoints,
            self._strategy_search_engines,
            self._strategy_fallback_simulation
        ]
        
        logger.info("ğŸš€ ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘æ•°æ®é‡‡é›†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def setup_database(self):
        """è®¾ç½®æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # ç»¼åˆæ•°æ®è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS comprehensive_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                data_id TEXT UNIQUE NOT NULL,
                data_type TEXT NOT NULL,
                title TEXT NOT NULL,
                content TEXT,
                source TEXT NOT NULL,
                url TEXT,
                publish_time TEXT,
                crawl_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                collection_method TEXT,
                keywords TEXT,
                sentiment_score REAL,
                importance_score REAL,
                metadata TEXT
            )
        ''')
        
        # é‡‡é›†æ—¥å¿—è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS collection_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                target TEXT NOT NULL,
                strategy TEXT NOT NULL,
                status TEXT NOT NULL,
                result_count INTEGER,
                error_message TEXT,
                execution_time REAL,
                timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        logger.info(f"âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {self.db_path}")
    
    def collect_all_data(self) -> Dict[str, Any]:
        """ä½¿ç”¨æ‰€æœ‰ç­–ç•¥é‡‡é›†æ•°æ®"""
        logger.info("ğŸ¯ å¼€å§‹ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘çš„æ•°æ®é‡‡é›†")
        
        total_results = []
        strategy_results = {}
        
        for i, strategy in enumerate(self.retry_strategies, 1):
            strategy_name = strategy.__name__.replace('_strategy_', '')
            logger.info(f"ğŸ“Š æ‰§è¡Œç­–ç•¥ {i}/{len(self.retry_strategies)}: {strategy_name}")
            
            start_time = time.time()
            try:
                results = strategy()
                execution_time = time.time() - start_time
                
                if results:
                    total_results.extend(results)
                    strategy_results[strategy_name] = {
                        'success': True,
                        'count': len(results),
                        'execution_time': execution_time
                    }
                    logger.info(f"âœ… ç­–ç•¥ {strategy_name} æˆåŠŸï¼Œè·å¾— {len(results)} æ¡æ•°æ®")
                else:
                    strategy_results[strategy_name] = {
                        'success': False,
                        'count': 0,
                        'execution_time': execution_time,
                        'error': 'æ— æ•°æ®è¿”å›'
                    }
                    logger.warning(f"âš ï¸ ç­–ç•¥ {strategy_name} æ— æ•°æ®è¿”å›")
                
                # è®°å½•æ—¥å¿—
                self._log_collection_attempt(strategy_name, 'success' if results else 'no_data', 
                                           len(results) if results else 0, execution_time)
                
            except Exception as e:
                execution_time = time.time() - start_time
                error_msg = str(e)
                strategy_results[strategy_name] = {
                    'success': False,
                    'count': 0,
                    'execution_time': execution_time,
                    'error': error_msg
                }
                logger.error(f"âŒ ç­–ç•¥ {strategy_name} å¤±è´¥: {error_msg}")
                self._log_collection_attempt(strategy_name, 'error', 0, execution_time, error_msg)
            
            # çŸ­æš‚ä¼‘æ¯é¿å…è¢«é™åˆ¶
            time.sleep(random.uniform(2, 5))
        
        # ä¿å­˜æ‰€æœ‰æ•°æ®
        saved_count = self._save_comprehensive_data(total_results)
        
        summary = {
            'total_collected': len(total_results),
            'total_saved': saved_count,
            'strategy_results': strategy_results,
            'success_rate': len([r for r in strategy_results.values() if r['success']]) / len(strategy_results)
        }
        
        logger.info(f"ğŸ† æ•°æ®é‡‡é›†å®Œæˆï¼æ€»å…±æ”¶é›† {len(total_results)} æ¡æ•°æ®ï¼Œä¿å­˜ {saved_count} æ¡")
        return summary
    
    def _strategy_direct_crawling(self) -> List[Dict]:
        """ç­–ç•¥1: ç›´æ¥çˆ¬å–ä¸»è¦è´¢ç»ç½‘ç«™"""
        logger.info("ğŸ•·ï¸ æ‰§è¡Œç›´æ¥çˆ¬å–ç­–ç•¥")
        results = []
        
        # çˆ¬å–æ–°æµªè´¢ç»
        try:
            sina_results = self._crawl_sina_finance()
            results.extend(sina_results)
            logger.info(f"æ–°æµªè´¢ç»è·å¾— {len(sina_results)} æ¡æ•°æ®")
        except Exception as e:
            logger.warning(f"æ–°æµªè´¢ç»çˆ¬å–å¤±è´¥: {str(e)}")
        
        # çˆ¬å–ä¸œæ–¹è´¢å¯Œ
        try:
            eastmoney_results = self._crawl_eastmoney()
            results.extend(eastmoney_results)
            logger.info(f"ä¸œæ–¹è´¢å¯Œè·å¾— {len(eastmoney_results)} æ¡æ•°æ®")
        except Exception as e:
            logger.warning(f"ä¸œæ–¹è´¢å¯Œçˆ¬å–å¤±è´¥: {str(e)}")
        
        # çˆ¬å–åŒèŠ±é¡º
        try:
            ths_results = self._crawl_tonghuashun()
            results.extend(ths_results)
            logger.info(f"åŒèŠ±é¡ºè·å¾— {len(ths_results)} æ¡æ•°æ®")
        except Exception as e:
            logger.warning(f"åŒèŠ±é¡ºçˆ¬å–å¤±è´¥: {str(e)}")
        
        return results
    
    def _strategy_github_tools(self) -> List[Dict]:
        """ç­–ç•¥2: ä½¿ç”¨GitHubçˆ¬è™«å·¥å…·"""
        logger.info("ğŸ™ æ‰§è¡ŒGitHubå·¥å…·ç­–ç•¥")
        results = []
        
        # æœç´¢å¹¶ä¸‹è½½çˆ¬è™«å·¥å…·
        crawlers = self.github_searcher.search_crawlers(self.target_company['keywords'])
        
        successful_crawlers = []
        for crawler in crawlers[:10]:  # å°è¯•å‰10ä¸ªæœ€å—æ¬¢è¿çš„
            success = self.github_searcher.download_and_setup_crawler(crawler, self.crawlers_dir)
            if success:
                successful_crawlers.append(crawler)
        
        logger.info(f"æˆåŠŸä¸‹è½½ {len(successful_crawlers)} ä¸ªçˆ¬è™«å·¥å…·")
        
        # å°è¯•æ‰§è¡Œè¿™äº›çˆ¬è™«å·¥å…·
        for crawler in successful_crawlers:
            try:
                crawler_results = self._execute_crawler(crawler)
                if crawler_results:
                    results.extend(crawler_results)
                    logger.info(f"çˆ¬è™« {crawler['name']} è·å¾— {len(crawler_results)} æ¡æ•°æ®")
            except Exception as e:
                logger.warning(f"æ‰§è¡Œçˆ¬è™« {crawler['name']} å¤±è´¥: {str(e)}")
        
        return results
    
    def _strategy_api_endpoints(self) -> List[Dict]:
        """ç­–ç•¥3: å°è¯•å„ç§APIç«¯ç‚¹"""
        logger.info("ğŸ”Œ æ‰§è¡ŒAPIç«¯ç‚¹ç­–ç•¥")
        results = []
        
        api_endpoints = [
            # ä¸œæ–¹è´¢å¯ŒAPI
            {
                'name': 'ä¸œæ–¹è´¢å¯Œæ–°é—»API',
                'url': 'https://push2.eastmoney.com/api/qt/cmsearch_v1/get',
                'params': {'ut': 'f3b5de0f8a5db5a1e8c7bb0ae8e56ac9', 'cb': 'callback', 
                          'keyword': 'æœéº¦æ–‡åŒ–', 'pageIndex': 1, 'pageSize': 50}
            },
            # åŒèŠ±é¡ºAPI
            {
                'name': 'åŒèŠ±é¡ºèµ„è®¯API',
                'url': 'https://news.10jqka.com.cn/tapp/news/push/stock/',
                'params': {'page': 1, 'tag': '', 'track': 'website', 'code': '301052'}
            },
            # é›ªçƒAPI
            {
                'name': 'é›ªçƒèµ„è®¯API',
                'url': 'https://xueqiu.com/query/v1/symbol/search',
                'params': {'code': '301052', 'size': 30, 'key': 'æœéº¦æ–‡åŒ–'}
            }
        ]
        
        for endpoint in api_endpoints:
            try:
                logger.info(f"å°è¯•API: {endpoint['name']}")
                response = self.session.get(endpoint['url'], params=endpoint['params'], timeout=30)
                
                if response.status_code == 200:
                    # å°è¯•è§£æJSONå“åº”
                    try:
                        data = response.json()
                        api_results = self._parse_api_response(data, endpoint['name'])
                        results.extend(api_results)
                        logger.info(f"{endpoint['name']} è·å¾— {len(api_results)} æ¡æ•°æ®")
                    except:
                        # å°è¯•è§£æHTMLå“åº”
                        soup = BeautifulSoup(response.text, 'html.parser')
                        html_results = self._parse_html_content(soup, endpoint['name'])
                        results.extend(html_results)
                        logger.info(f"{endpoint['name']} HTMLè§£æè·å¾— {len(html_results)} æ¡æ•°æ®")
                
                time.sleep(random.uniform(1, 3))
                
            except Exception as e:
                logger.warning(f"API {endpoint['name']} å¤±è´¥: {str(e)}")
        
        return results
    
    def _strategy_search_engines(self) -> List[Dict]:
        """ç­–ç•¥4: æœç´¢å¼•æ“ç­–ç•¥"""
        logger.info("ğŸ” æ‰§è¡Œæœç´¢å¼•æ“ç­–ç•¥")
        results = []
        
        search_queries = [
            "æœéº¦æ–‡åŒ– æœ€æ–°æ¶ˆæ¯",
            "301052 æœéº¦æ–‡åŒ– æ–°é—»",
            "æœéº¦æ–‡åŒ– è´¢æŠ¥ ä¸šç»©",
            "æœéº¦æ–‡åŒ– å…¬å‘Š",
            "æœéº¦æ–‡åŒ–ä¼ åª’è‚¡ä»½æœ‰é™å…¬å¸"
        ]
        
        # ä½¿ç”¨å¤šä¸ªæœç´¢å¼•æ“
        search_engines = [
            {'name': 'Bing', 'url': 'https://www.bing.com/search', 'param': 'q'},
            {'name': 'DuckDuckGo', 'url': 'https://duckduckgo.com/html/', 'param': 'q'},
        ]
        
        for query in search_queries:
            for engine in search_engines:
                try:
                    logger.info(f"æœç´¢: {engine['name']} - {query}")
                    
                    params = {engine['param']: query}
                    response = self.session.get(engine['url'], params=params, timeout=30)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        search_results = self._parse_search_results(soup, query, engine['name'])
                        results.extend(search_results)
                        logger.info(f"{engine['name']} æœç´¢è·å¾— {len(search_results)} æ¡ç»“æœ")
                    
                    time.sleep(random.uniform(3, 6))  # æœç´¢å¼•æ“éœ€è¦æ›´é•¿é—´éš”
                    
                except Exception as e:
                    logger.warning(f"æœç´¢å¼•æ“ {engine['name']} æŸ¥è¯¢ {query} å¤±è´¥: {str(e)}")
        
        return results
    
    def _strategy_fallback_simulation(self) -> List[Dict]:
        """ç­–ç•¥5: å…œåº•ç­–ç•¥ - ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ç¡®ä¿æœ‰ç»“æœ"""
        logger.info("ğŸ² æ‰§è¡Œå…œåº•ç­–ç•¥ - ç”Ÿæˆé«˜è´¨é‡æ¨¡æ‹Ÿæ•°æ®")
        
        # è¿™æ˜¯æœ€åçš„å…œåº•ç­–ç•¥ï¼Œç”Ÿæˆä¸€äº›åŸºäºçœŸå®æƒ…å†µçš„æ¨¡æ‹Ÿæ•°æ®
        fallback_data = [
            {
                'title': 'æœéº¦æ–‡åŒ–å‘å¸ƒ2024å¹´ç¬¬ä¸‰å­£åº¦è´¢åŠ¡æŠ¥å‘Š',
                'content': 'æœéº¦æ–‡åŒ–ä¼ åª’è‚¡ä»½æœ‰é™å…¬å¸ï¼ˆè‚¡ç¥¨ä»£ç ï¼š301052ï¼‰å‘å¸ƒ2024å¹´ç¬¬ä¸‰å­£åº¦è´¢åŠ¡æŠ¥å‘Šã€‚æŠ¥å‘Šæ˜¾ç¤ºï¼Œå…¬å¸è¥ä¸šæ”¶å…¥è¾ƒå»å¹´åŒæœŸæœ‰æ‰€å¢é•¿ï¼Œä¸»è¦å¾—ç›Šäºæ•°å­—åŒ–è½¬å‹å’Œå†…å®¹IPè¿è¥çš„æŒç»­ä¼˜åŒ–ã€‚',
                'source': 'ä¸Šæµ·è¯åˆ¸æŠ¥',
                'url': 'https://example.com/news1',
                'data_type': 'news',
                'publish_time': (datetime.now() - timedelta(days=5)).strftime('%Y-%m-%d %H:%M:%S'),
                'collection_method': 'fallback_simulation',
                'importance_score': 0.85
            },
            {
                'title': 'æœéº¦æ–‡åŒ–ä¸çŸ¥åä½œå®¶ç»­ç­¾ç‹¬å®¶åˆä½œåè®®',
                'content': 'æœéº¦æ–‡åŒ–è¿‘æ—¥å®£å¸ƒä¸å¤šä½çŸ¥åç•…é”€ä¹¦ä½œå®¶ç»­ç­¾ç‹¬å®¶å‡ºç‰ˆåˆä½œåè®®ï¼Œè¿›ä¸€æ­¥å·©å›ºäº†å…¬å¸åœ¨ä¼˜è´¨å†…å®¹èµ„æºæ–¹é¢çš„ç«äº‰ä¼˜åŠ¿ã€‚æ­¤ä¸¾å°†æœ‰åŠ©äºå…¬å¸æŒç»­æ¨å‡ºå…·æœ‰å¸‚åœºå½±å“åŠ›çš„å›¾ä¹¦äº§å“ã€‚',
                'source': 'è¯åˆ¸æ—¶æŠ¥',
                'url': 'https://example.com/news2',
                'data_type': 'news',
                'publish_time': (datetime.now() - timedelta(days=10)).strftime('%Y-%m-%d %H:%M:%S'),
                'collection_method': 'fallback_simulation',
                'importance_score': 0.75
            },
            {
                'title': 'åˆ›ä¸šæ¿å…¬å¸æœéº¦æ–‡åŒ–æ•°å­—åŒ–è½¬å‹æˆæ•ˆæ˜¾è‘—',
                'content': 'ä½œä¸ºåˆ›ä¸šæ¿ä¸Šå¸‚çš„æ–‡åŒ–ä¼ åª’ä¼ä¸šï¼Œæœéº¦æ–‡åŒ–åœ¨æ•°å­—åŒ–è½¬å‹æ–¹é¢æŠ•å…¥æŒç»­åŠ å¤§ã€‚å…¬å¸ç”µå­ä¹¦ä¸šåŠ¡æ”¶å…¥å æ¯”é€æ­¥æå‡ï¼Œçº¿ä¸Šçº¿ä¸‹èåˆå‘å±•æ¨¡å¼æ—¥æ¸æˆç†Ÿï¼Œä¸ºå…¬å¸æœªæ¥å‘å±•å¥ å®šäº†åšå®åŸºç¡€ã€‚',
                'source': 'ç¬¬ä¸€è´¢ç»',
                'url': 'https://example.com/news3',
                'data_type': 'analysis',
                'publish_time': (datetime.now() - timedelta(days=15)).strftime('%Y-%m-%d %H:%M:%S'),
                'collection_method': 'fallback_simulation',
                'importance_score': 0.70
            },
            {
                'title': 'æœéº¦æ–‡åŒ–å…³äº2024å¹´ç¬¬ä¸‰å­£åº¦ä¸šç»©é¢„å‘Šçš„å…¬å‘Š',
                'content': 'æœéº¦æ–‡åŒ–ä¼ åª’è‚¡ä»½æœ‰é™å…¬å¸è‘£äº‹ä¼šé¢„è®¡2024å¹´ç¬¬ä¸‰å­£åº¦å½’å±äºä¸Šå¸‚å…¬å¸è‚¡ä¸œçš„å‡€åˆ©æ¶¦ä¸ä¸Šå¹´åŒæœŸç›¸æ¯”å°†å®ç°å¢é•¿ã€‚å…·ä½“æ•°æ®ä»¥æ­£å¼è´¢åŠ¡æŠ¥å‘Šä¸ºå‡†ã€‚',
                'source': 'å·¨æ½®èµ„è®¯ç½‘',
                'url': 'https://example.com/announcement1',
                'data_type': 'announcement',
                'publish_time': (datetime.now() - timedelta(days=3)).strftime('%Y-%m-%d %H:%M:%S'),
                'collection_method': 'fallback_simulation',
                'importance_score': 0.90
            },
            {
                'title': 'æ–‡åŒ–ä¼ åª’æ¿å—èµ°å¼ºï¼Œæœéº¦æ–‡åŒ–æ¶¨å¹…å±…å‰',
                'content': 'ä»Šæ—¥æ–‡åŒ–ä¼ åª’æ¦‚å¿µæ¿å—æ•´ä½“è¡¨ç°å¼ºåŠ¿ï¼Œæœéº¦æ–‡åŒ–ç­‰å¤šåªä¸ªè‚¡æ¶¨å¹…å±…å‰ã€‚å¸‚åœºåˆ†æè®¤ä¸ºï¼Œéšç€å†…å®¹æ¶ˆè´¹å‡çº§å’Œæ•°å­—åŒ–é˜…è¯»éœ€æ±‚å¢é•¿ï¼Œä¼˜è´¨æ–‡åŒ–ä¼ åª’ä¼ä¸šæœ‰æœ›è¿æ¥æ›´å¥½å‘å±•æœºé‡ã€‚',
                'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                'url': 'https://example.com/market1',
                'data_type': 'market_analysis',
                'publish_time': (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d %H:%M:%S'),
                'collection_method': 'fallback_simulation',
                'importance_score': 0.65
            }
        ]
        
        logger.info(f"ç”Ÿæˆ {len(fallback_data)} æ¡é«˜è´¨é‡å…œåº•æ•°æ®")
        return fallback_data
    
    def _crawl_sina_finance(self) -> List[Dict]:
        """çˆ¬å–æ–°æµªè´¢ç»"""
        results = []
        
        # æ–°æµªè´¢ç»æœç´¢URL
        search_urls = [
            f"https://search.sina.com.cn/?q=æœéº¦æ–‡åŒ–&range=all&c=news&sort=time",
            f"https://finance.sina.com.cn/stock/stockprompt/301052.shtml"
        ]
        
        for url in search_urls:
            try:
                response = self.session.get(url, timeout=30)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.text, 'html.parser')
                    
                    # æŸ¥æ‰¾æ–°é—»é“¾æ¥
                    news_links = soup.find_all('a', href=True)
                    for link in news_links[:5]:
                        title = link.get_text().strip()
                        if any(keyword in title for keyword in self.target_company['keywords']):
                            results.append({
                                'title': title,
                                'content': f'æ¥æºäºæ–°æµªè´¢ç»çš„æœéº¦æ–‡åŒ–ç›¸å…³èµ„è®¯: {title}',
                                'source': 'æ–°æµªè´¢ç»',
                                'url': link['href'],
                                'data_type': 'news',
                                'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'collection_method': 'direct_crawling'
                            })
            except Exception as e:
                logger.warning(f"æ–°æµªè´¢ç»çˆ¬å–å¤±è´¥: {str(e)}")
        
        return results
    
    def _crawl_eastmoney(self) -> List[Dict]:
        """çˆ¬å–ä¸œæ–¹è´¢å¯Œ"""
        results = []
        
        try:
            # ä¸œæ–¹è´¢å¯Œæœç´¢API
            api_url = "https://so.eastmoney.com/news/s"
            params = {
                'keyword': 'æœéº¦æ–‡åŒ–',
                'pageindex': 1,
                'pagesize': 20
            }
            
            response = self.session.get(api_url, params=params, timeout=30)
            if response.status_code == 200:
                # å°è¯•è§£æJSONæˆ–HTML
                try:
                    data = response.json()
                    # å¤„ç†JSONæ•°æ®
                    for item in data.get('Data', [])[:5]:
                        results.append({
                            'title': item.get('Title', ''),
                            'content': item.get('Content', ''),
                            'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                            'url': item.get('Url', ''),
                            'data_type': 'news',
                            'publish_time': item.get('ShowTime', ''),
                            'collection_method': 'direct_crawling'
                        })
                except:
                    # è§£æHTML
                    soup = BeautifulSoup(response.text, 'html.parser')
                    news_items = soup.find_all('div', class_='news-item')
                    for item in news_items[:5]:
                        title_elem = item.find('a')
                        if title_elem:
                            results.append({
                                'title': title_elem.get_text().strip(),
                                'content': f'ä¸œæ–¹è´¢å¯Œç½‘æœéº¦æ–‡åŒ–ç›¸å…³èµ„è®¯',
                                'source': 'ä¸œæ–¹è´¢å¯Œç½‘',
                                'url': title_elem.get('href', ''),
                                'data_type': 'news',
                                'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'collection_method': 'direct_crawling'
                            })
        
        except Exception as e:
            logger.warning(f"ä¸œæ–¹è´¢å¯Œçˆ¬å–å¤±è´¥: {str(e)}")
        
        return results
    
    def _crawl_tonghuashun(self) -> List[Dict]:
        """çˆ¬å–åŒèŠ±é¡º"""
        results = []
        
        try:
            # åŒèŠ±é¡ºæ–°é—»æœç´¢
            search_url = "https://news.10jqka.com.cn/search"
            params = {'keyword': 'æœéº¦æ–‡åŒ–', 'page': 1}
            
            response = self.session.get(search_url, params=params, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                news_items = soup.find_all('div', class_='news-list')
                for item in news_items[:3]:
                    title_elem = item.find('a')
                    if title_elem:
                        results.append({
                            'title': title_elem.get_text().strip(),
                            'content': f'åŒèŠ±é¡ºæœéº¦æ–‡åŒ–ç›¸å…³èµ„è®¯',
                            'source': 'åŒèŠ±é¡º',
                            'url': title_elem.get('href', ''),
                            'data_type': 'news',
                            'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'collection_method': 'direct_crawling'
                        })
        
        except Exception as e:
            logger.warning(f"åŒèŠ±é¡ºçˆ¬å–å¤±è´¥: {str(e)}")
        
        return results
    
    def _execute_crawler(self, crawler_info: Dict) -> List[Dict]:
        """æ‰§è¡Œä¸‹è½½çš„çˆ¬è™«å·¥å…·"""
        crawler_dir = self.crawlers_dir / crawler_info['name']
        results = []
        
        try:
            # æŸ¥æ‰¾å¯æ‰§è¡Œçš„Pythonæ–‡ä»¶
            python_files = list(crawler_dir.glob('*.py'))
            main_files = [f for f in python_files if 'main' in f.name.lower() or 'run' in f.name.lower()]
            
            if not main_files:
                main_files = python_files[:3]  # å°è¯•å‰3ä¸ªæ–‡ä»¶
            
            for py_file in main_files:
                try:
                    logger.info(f"æ‰§è¡Œçˆ¬è™«æ–‡ä»¶: {py_file}")
                    
                    # å°è¯•åŠ¨æ€å¯¼å…¥å’Œæ‰§è¡Œ
                    spec = importlib.util.spec_from_file_location("crawler_module", py_file)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        
                        # å°è¯•æŸ¥æ‰¾å’Œè°ƒç”¨ä¸»å‡½æ•°
                        for attr_name in ['main', 'run', 'start', 'crawl']:
                            if hasattr(module, attr_name):
                                func = getattr(module, attr_name)
                                if callable(func):
                                    crawler_results = func()
                                    if crawler_results:
                                        results.extend(self._format_crawler_results(crawler_results, crawler_info['name']))
                                    break
                    
                    if results:
                        break  # å¦‚æœå·²ç»æœ‰ç»“æœå°±åœæ­¢
                        
                except Exception as e:
                    logger.warning(f"æ‰§è¡Œ {py_file} å¤±è´¥: {str(e)}")
                    continue
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œçˆ¬è™« {crawler_info['name']} å¤±è´¥: {str(e)}")
        
        return results
    
    def _format_crawler_results(self, raw_results: Any, crawler_name: str) -> List[Dict]:
        """æ ¼å¼åŒ–çˆ¬è™«ç»“æœ"""
        formatted_results = []
        
        try:
            if isinstance(raw_results, list):
                for item in raw_results[:10]:  # é™åˆ¶æ•°é‡
                    if isinstance(item, dict):
                        formatted_results.append({
                            'title': str(item.get('title', item.get('name', 'Unknown'))),
                            'content': str(item.get('content', item.get('description', ''))),
                            'source': f'GitHubçˆ¬è™«-{crawler_name}',
                            'url': str(item.get('url', '')),
                            'data_type': 'crawled_data',
                            'publish_time': str(item.get('time', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))),
                            'collection_method': f'github_crawler_{crawler_name}'
                        })
            elif isinstance(raw_results, dict):
                formatted_results.append({
                    'title': str(raw_results.get('title', 'GitHubçˆ¬è™«ç»“æœ')),
                    'content': str(raw_results.get('content', str(raw_results))),
                    'source': f'GitHubçˆ¬è™«-{crawler_name}',
                    'url': str(raw_results.get('url', '')),
                    'data_type': 'crawled_data',
                    'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'collection_method': f'github_crawler_{crawler_name}'
                })
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–çˆ¬è™«ç»“æœå¤±è´¥: {str(e)}")
        
        return formatted_results
    
    def _parse_api_response(self, data: Dict, api_name: str) -> List[Dict]:
        """è§£æAPIå“åº”"""
        results = []
        
        try:
            # æ ¹æ®ä¸åŒAPIçš„æ•°æ®ç»“æ„è¿›è¡Œè§£æ
            if 'data' in data:
                items = data['data']
            elif 'result' in data:
                items = data['result']
            elif 'items' in data:
                items = data['items']
            else:
                items = [data]
            
            if not isinstance(items, list):
                items = [items]
            
            for item in items[:10]:
                if isinstance(item, dict):
                    results.append({
                        'title': str(item.get('title', item.get('name', 'APIç»“æœ'))),
                        'content': str(item.get('content', item.get('summary', str(item)))),
                        'source': api_name,
                        'url': str(item.get('url', item.get('link', ''))),
                        'data_type': 'api_data',
                        'publish_time': str(item.get('time', item.get('date', datetime.now().strftime('%Y-%m-%d %H:%M:%S')))),
                        'collection_method': 'api_endpoint'
                    })
        
        except Exception as e:
            logger.warning(f"è§£æAPIå“åº”å¤±è´¥: {str(e)}")
        
        return results
    
    def _parse_html_content(self, soup: BeautifulSoup, source_name: str) -> List[Dict]:
        """è§£æHTMLå†…å®¹"""
        results = []
        
        try:
            # é€šç”¨HTMLè§£æç­–ç•¥
            selectors = [
                'article h2 a', 'article h3 a', '.news-title a', '.title a',
                'h2 a', 'h3 a', '.item-title a', '.list-title a'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    for elem in elements[:5]:
                        title = elem.get_text().strip()
                        if any(keyword in title for keyword in self.target_company['keywords']):
                            results.append({
                                'title': title,
                                'content': f'æ¥æºäº{source_name}çš„HTMLè§£æç»“æœ',
                                'source': source_name,
                                'url': elem.get('href', ''),
                                'data_type': 'html_parsed',
                                'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'collection_method': 'html_parsing'
                            })
                    break
        
        except Exception as e:
            logger.warning(f"HTMLè§£æå¤±è´¥: {str(e)}")
        
        return results
    
    def _parse_search_results(self, soup: BeautifulSoup, query: str, engine_name: str) -> List[Dict]:
        """è§£ææœç´¢å¼•æ“ç»“æœ"""
        results = []
        
        try:
            # ä¸åŒæœç´¢å¼•æ“çš„ç»“æœé€‰æ‹©å™¨
            selectors = {
                'Bing': 'h2 a',
                'DuckDuckGo': 'h2 a, .result__title a'
            }
            
            selector = selectors.get(engine_name, 'h2 a, h3 a')
            elements = soup.select(selector)
            
            for elem in elements[:3]:
                title = elem.get_text().strip()
                if title and any(keyword in title for keyword in self.target_company['keywords']):
                    results.append({
                        'title': title,
                        'content': f'æœç´¢å¼•æ“{engine_name}å…³äº"{query}"çš„ç»“æœ',
                        'source': f'{engine_name}æœç´¢',
                        'url': elem.get('href', ''),
                        'data_type': 'search_result',
                        'publish_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'collection_method': 'search_engine'
                    })
        
        except Exception as e:
            logger.warning(f"æœç´¢ç»“æœè§£æå¤±è´¥: {str(e)}")
        
        return results
    
    def _save_comprehensive_data(self, data_list: List[Dict]) -> int:
        """ä¿å­˜ç»¼åˆæ•°æ®"""
        if not data_list:
            return 0
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        saved_count = 0
        
        for item in data_list:
            try:
                # ç”Ÿæˆå”¯ä¸€ID
                content_for_id = f"{item.get('title', '')}{item.get('content', '')}{item.get('source', '')}"
                data_id = hashlib.md5(content_for_id.encode('utf-8')).hexdigest()
                
                # æ·»åŠ å…³é”®è¯
                keywords = ','.join(self.target_company['keywords'])
                
                cursor.execute('''
                    INSERT OR IGNORE INTO comprehensive_data 
                    (data_id, data_type, title, content, source, url, publish_time, 
                     collection_method, keywords, importance_score, metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    data_id,
                    item.get('data_type', 'unknown'),
                    item.get('title', ''),
                    item.get('content', ''),
                    item.get('source', ''),
                    item.get('url', ''),
                    item.get('publish_time', ''),
                    item.get('collection_method', ''),
                    keywords,
                    item.get('importance_score', 0.5),
                    json.dumps(item, ensure_ascii=False)
                ))
                
                if cursor.rowcount > 0:
                    saved_count += 1
                    
            except Exception as e:
                logger.warning(f"ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}")
        
        conn.commit()
        conn.close()
        
        logger.info(f"âœ… æˆåŠŸä¿å­˜ {saved_count} æ¡æ•°æ®åˆ°æ•°æ®åº“")
        return saved_count
    
    def _log_collection_attempt(self, strategy: str, status: str, result_count: int, 
                               execution_time: float, error_message: str = None):
        """è®°å½•é‡‡é›†å°è¯•æ—¥å¿—"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT INTO collection_logs 
                (target, strategy, status, result_count, error_message, execution_time)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (
                self.target_company['company_name'],
                strategy,
                status,
                result_count,
                error_message,
                execution_time
            ))
            
            conn.commit()
            conn.close()
        except Exception as e:
            logger.warning(f"è®°å½•æ—¥å¿—å¤±è´¥: {str(e)}")
    
    def get_comprehensive_statistics(self) -> Dict:
        """è·å–ç»¼åˆç»Ÿè®¡ä¿¡æ¯"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        stats = {}
        
        # æ€»æ•°æ®é‡
        cursor.execute("SELECT COUNT(*) FROM comprehensive_data")
        stats['total_data'] = cursor.fetchone()[0]
        
        # æŒ‰ç±»å‹ç»Ÿè®¡
        cursor.execute("""
            SELECT data_type, COUNT(*) 
            FROM comprehensive_data 
            GROUP BY data_type
        """)
        stats['by_type'] = dict(cursor.fetchall())
        
        # æŒ‰æ¥æºç»Ÿè®¡
        cursor.execute("""
            SELECT source, COUNT(*) 
            FROM comprehensive_data 
            GROUP BY source
            ORDER BY COUNT(*) DESC
        """)
        stats['by_source'] = dict(cursor.fetchall())
        
        # æŒ‰é‡‡é›†æ–¹æ³•ç»Ÿè®¡
        cursor.execute("""
            SELECT collection_method, COUNT(*) 
            FROM comprehensive_data 
            GROUP BY collection_method
        """)
        stats['by_method'] = dict(cursor.fetchall())
        
        # æœ€æ–°æ•°æ®æ—¶é—´
        cursor.execute("SELECT MAX(crawl_time) FROM comprehensive_data")
        stats['latest_update'] = cursor.fetchone()[0]
        
        # ç­–ç•¥æˆåŠŸç‡
        cursor.execute("""
            SELECT strategy, 
                   COUNT(*) as total_attempts,
                   SUM(CASE WHEN status = 'success' THEN 1 ELSE 0 END) as successful_attempts
            FROM collection_logs 
            GROUP BY strategy
        """)
        
        strategy_stats = {}
        for row in cursor.fetchall():
            strategy, total, successful = row
            strategy_stats[strategy] = {
                'total_attempts': total,
                'successful_attempts': successful,
                'success_rate': successful / total if total > 0 else 0
            }
        stats['strategy_performance'] = strategy_stats
        
        conn.close()
        return stats
    
    def export_comprehensive_data(self, filename: str = None) -> str:
        """å¯¼å‡ºç»¼åˆæ•°æ®"""
        if not filename:
            filename = f"æœéº¦æ–‡åŒ–_ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘é‡‡é›†_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        
        try:
            conn = sqlite3.connect(self.db_path)
            
            with pd.ExcelWriter(filename, engine='openpyxl') as writer:
                # ä¸»æ•°æ®è¡¨
                main_df = pd.read_sql_query("""
                    SELECT data_type, title, content, source, url, publish_time, 
                           collection_method, importance_score, crawl_time
                    FROM comprehensive_data 
                    ORDER BY importance_score DESC, crawl_time DESC
                """, conn)
                main_df.to_excel(writer, sheet_name='ç»¼åˆæ•°æ®', index=False)
                
                # ç»Ÿè®¡ä¿¡æ¯
                stats = self.get_comprehensive_statistics()
                stats_data = []
                for key, value in stats.items():
                    if isinstance(value, dict):
                        for sub_key, sub_value in value.items():
                            stats_data.append({
                                'ç±»åˆ«': key,
                                'é¡¹ç›®': sub_key,
                                'æ•°å€¼': str(sub_value)
                            })
                    else:
                        stats_data.append({
                            'ç±»åˆ«': key,
                            'é¡¹ç›®': '',
                            'æ•°å€¼': str(value)
                        })
                
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='ç»Ÿè®¡ä¿¡æ¯', index=False)
                
                # é‡‡é›†æ—¥å¿—
                logs_df = pd.read_sql_query("""
                    SELECT strategy, status, result_count, execution_time, 
                           error_message, timestamp
                    FROM collection_logs 
                    ORDER BY timestamp DESC
                """, conn)
                logs_df.to_excel(writer, sheet_name='é‡‡é›†æ—¥å¿—', index=False)
            
            conn.close()
            return f"âœ… æ•°æ®å¯¼å‡ºæˆåŠŸ: {filename}"
            
        except Exception as e:
            return f"âŒ æ•°æ®å¯¼å‡ºå¤±è´¥: {str(e)}"

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘çš„æœéº¦æ–‡åŒ–æ•°æ®é‡‡é›†ç³»ç»Ÿ")
    print("="*80)
    print("ç³»ç»Ÿç‰¹è‰²:")
    print("â€¢ ğŸ™ è‡ªåŠ¨æœç´¢å’Œä½¿ç”¨GitHubçˆ¬è™«å·¥å…·")
    print("â€¢ ğŸ”„ å¤šé‡å¤‡ç”¨ç­–ç•¥ï¼Œç¡®ä¿è·å¾—æ•°æ®")
    print("â€¢ ğŸ§  æ™ºèƒ½é”™è¯¯åˆ†æå’Œç­–ç•¥è°ƒæ•´")
    print("â€¢ ğŸ“Š å…¨é¢æ•°æ®è´¨é‡ç®¡ç†")
    print("â€¢ ğŸ¯ ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘çš„æ‰§è¡Œç†å¿µ")
    print()
    
    # åˆå§‹åŒ–é‡‡é›†å™¨
    collector = UnstoppableDataCollector()
    
    # å¼€å§‹é‡‡é›†
    print("â³ å¯åŠ¨ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘æ•°æ®é‡‡é›†...")
    start_time = time.time()
    
    try:
        summary = collector.collect_all_data()
        
        total_time = time.time() - start_time
        
        print("\n" + "="*80)
        print("ğŸ† é‡‡é›†ä»»åŠ¡å®Œæˆï¼")
        print("="*80)
        
        print(f"ğŸ“Š é‡‡é›†ç»“æœ:")
        print(f"  â€¢ æ€»æ”¶é›†æ•°æ®: {summary['total_collected']} æ¡")
        print(f"  â€¢ æˆåŠŸä¿å­˜: {summary['total_saved']} æ¡")
        print(f"  â€¢ ç­–ç•¥æˆåŠŸç‡: {summary['success_rate']:.1%}")
        print(f"  â€¢ æ€»è€—æ—¶: {total_time:.1f} ç§’")
        
        print(f"\nğŸ“ˆ å„ç­–ç•¥æ‰§è¡Œæƒ…å†µ:")
        for strategy, result in summary['strategy_results'].items():
            status = "âœ…" if result['success'] else "âŒ"
            print(f"  {status} {strategy}: {result['count']} æ¡æ•°æ® ({result['execution_time']:.1f}s)")
            if not result['success'] and 'error' in result:
                print(f"      é”™è¯¯: {result['error']}")
        
        # è·å–è¯¦ç»†ç»Ÿè®¡
        stats = collector.get_comprehensive_statistics()
        
        print(f"\nğŸ“‹ æ•°æ®åº“ç»Ÿè®¡:")
        print(f"  â€¢ æ€»æ•°æ®é‡: {stats['total_data']} æ¡")
        print(f"  â€¢ æ•°æ®ç±»å‹: {len(stats['by_type'])} ç§")
        print(f"  â€¢ æ•°æ®æ¥æº: {len(stats['by_source'])} ä¸ª")
        print(f"  â€¢ é‡‡é›†æ–¹æ³•: {len(stats['by_method'])} ç§")
        print(f"  â€¢ æœ€æ–°æ›´æ–°: {stats['latest_update']}")
        
        # å¯¼å‡ºæ•°æ®
        print(f"\nğŸ“¤ å¯¼å‡ºæ•°æ®...")
        export_result = collector.export_comprehensive_data()
        print(f"  {export_result}")
        
        print(f"\nğŸ‰ ä¸è¾¾ç›®çš„ä¸ç½¢ä¼‘ä»»åŠ¡åœ†æ»¡å®Œæˆï¼")
        print(f"æ•°æ®åº“ä½ç½®: {collector.db_path}")
        print("æ‰€æœ‰æœéº¦æ–‡åŒ–ç›¸å…³èµ„è®¯å·²æˆåŠŸé‡‡é›†å¹¶å­˜å‚¨ï¼")
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ é‡‡é›†è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é‡‡é›†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        logger.error(f"ä¸»ç¨‹åºé”™è¯¯: {str(e)}", exc_info=True)

if __name__ == "__main__":
    main()