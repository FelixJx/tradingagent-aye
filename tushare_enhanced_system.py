#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºçœŸå®tushareæ•°æ®çš„å¢å¼ºå› å­ç³»ç»Ÿ
ç»“åˆqlibæœ¬åœ°æ•°æ®å’Œtushareå®æ—¶æ•°æ®
"""

import tushare as ts
import pandas as pd
import numpy as np
import sqlite3
import os
from datetime import datetime, timedelta
import time

class TushareEnhancedFactorSystem:
    """
    ç»“åˆtushareå’Œqlibçš„å¢å¼ºå› å­ç³»ç»Ÿ
    """
    
    def __init__(self, tushare_token):
        print("ğŸš€ åˆå§‹åŒ–Tushareå¢å¼ºå› å­ç³»ç»Ÿ...")
        
        # åˆå§‹åŒ–tushare
        ts.set_token(tushare_token)
        self.pro = ts.pro_api()
        
        # qlibæ•°æ®åº“è·¯å¾„
        self.qlib_db_path = '/Users/jx/Downloads/qlib-main/databases/real_tushare_factor_analysis.db'
        
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def get_hybrid_stock_data(self, stock_code, start_date='20250701', end_date='20250731', use_tushare=True):
        """
        è·å–æ··åˆæ•°æ®ï¼šä¼˜å…ˆtushareå®æ—¶ï¼Œå¤‡ç”¨qlibå†å²
        """
        print(f"ğŸ” è·å– {stock_code} çš„æ··åˆæ•°æ®...")
        
        if use_tushare:
            try:
                # ä»tushareè·å–å®æ—¶æ•°æ®
                print("  ğŸ“¡ æ­£åœ¨ä»tushareè·å–å®æ—¶æ•°æ®...")
                df_tushare = self.pro.daily(ts_code=stock_code, start_date=start_date, end_date=end_date)
                
                if not df_tushare.empty:
                    df_tushare['trade_date'] = pd.to_datetime(df_tushare['trade_date'])
                    df_tushare = df_tushare.sort_values('trade_date').reset_index(drop=True)
                    
                    # è·å–åŸºæœ¬é¢æ•°æ®
                    latest_date = df_tushare.iloc[0]['trade_date'].strftime('%Y%m%d')
                    daily_basic = self.pro.daily_basic(ts_code=stock_code, trade_date=latest_date,
                                                     fields='ts_code,pe,pb,total_mv,turnover_rate')
                    
                    # æ·»åŠ åŸºæœ¬é¢ä¿¡æ¯
                    if not daily_basic.empty:
                        basic_info = daily_basic.iloc[0]
                        df_tushare['pe_ratio'] = basic_info['pe']
                        df_tushare['pb_ratio'] = basic_info['pb'] 
                        df_tushare['market_cap'] = basic_info['total_mv']
                        df_tushare['turnover_rate'] = basic_info['turnover_rate']
                    
                    print(f"  âœ… tushareæ•°æ®è·å–æˆåŠŸ: {len(df_tushare)} æ¡è®°å½•")
                    return df_tushare, 'tushare'
                    
            except Exception as e:
                print(f"  âš ï¸ tushareè·å–å¤±è´¥: {e}")
        
        # å¤‡ç”¨æ–¹æ¡ˆï¼šä»qlibæ•°æ®åº“è·å–
        try:
            print("  ğŸ—„ï¸ ä»qlibæ•°æ®åº“è·å–å¤‡ç”¨æ•°æ®...")
            conn = sqlite3.connect(self.qlib_db_path)
            
            query = """
            SELECT ts_code, trade_date, open, high, low, close, vol, amount, pct_chg
            FROM factor_data 
            WHERE ts_code = ? 
            ORDER BY trade_date DESC
            LIMIT 50
            """
            
            df_qlib = pd.read_sql_query(query, conn, params=[stock_code])
            conn.close()
            
            if not df_qlib.empty:
                df_qlib['trade_date'] = pd.to_datetime(df_qlib['trade_date'])
                print(f"  âœ… qlibæ•°æ®è·å–æˆåŠŸ: {len(df_qlib)} æ¡è®°å½•")
                return df_qlib, 'qlib'
                
        except Exception as e:
            print(f"  âŒ qlibæ•°æ®è·å–å¤±è´¥: {e}")
        
        return pd.DataFrame(), 'none'
    
    def calculate_comprehensive_factors(self, df, data_source='tushare'):
        """
        è®¡ç®—ç»¼åˆå› å­ï¼ˆæ¯”ä¹‹å‰æ›´å…¨é¢ï¼‰
        """
        print(f"âš™ï¸ è®¡ç®—ç»¼åˆå› å­ (æ•°æ®æº: {data_source})...")
        
        if df.empty:
            return {}
        
        factors = {}
        n = len(df)
        
        # 1. ä»·æ ¼åŠ¨é‡å› å­
        print("  ğŸ“ˆ è®¡ç®—ä»·æ ¼åŠ¨é‡å› å­...")
        for period in [1, 3, 5, 10, 20]:
            if period < n:
                factors[f'momentum_{period}d'] = (df['close'] - df['close'].shift(period)) / df['close'].shift(period)
                factors[f'return_{period}d'] = df['close'].pct_change(period)
        
        # 2. æ³¢åŠ¨ç‡å› å­
        print("  ğŸ“Š è®¡ç®—æ³¢åŠ¨ç‡å› å­...")
        returns = df['close'].pct_change()
        for period in [5, 10, 20]:
            if period < n:
                factors[f'volatility_{period}d'] = returns.rolling(period).std()
                factors[f'vol_rank_{period}d'] = factors[f'volatility_{period}d'].rank(pct=True)
        
        # 3. æŠ€æœ¯æŒ‡æ ‡å› å­
        print("  ğŸ”§ è®¡ç®—æŠ€æœ¯æŒ‡æ ‡å› å­...")
        
        # RSI
        for period in [14, 21]:
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0).rolling(period).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
            rs = gain / loss
            factors[f'rsi_{period}'] = 100 - (100 / (1 + rs))
        
        # ç§»åŠ¨å¹³å‡
        for period in [5, 10, 20, 60]:
            if period < n:
                ma = df['close'].rolling(period).mean()
                factors[f'ma_{period}'] = ma
                factors[f'ma_ratio_{period}'] = df['close'] / ma
                factors[f'ma_distance_{period}'] = (df['close'] - ma) / ma
        
        # 4. æˆäº¤é‡å› å­
        print("  ğŸ’° è®¡ç®—æˆäº¤é‡å› å­...")
        for period in [5, 10, 20]:
            if period < n:
                vol_ma = df['vol'].rolling(period).mean()
                factors[f'volume_ma_{period}'] = vol_ma
                factors[f'volume_ratio_{period}'] = df['vol'] / vol_ma
        
        # é‡ä»·ç›¸å…³æ€§
        for period in [10, 20]:
            if period < n:
                factors[f'volume_price_corr_{period}'] = returns.rolling(period).corr(df['vol'].pct_change())
        
        # 5. ä»·æ ¼ä½ç½®å› å­
        print("  ğŸ“ è®¡ç®—ä»·æ ¼ä½ç½®å› å­...")
        for period in [10, 20, 60]:
            if period < n:
                high_max = df['high'].rolling(period).max()
                low_min = df['low'].rolling(period).min()
                factors[f'price_position_{period}'] = (df['close'] - low_min) / (high_max - low_min)
                factors[f'price_percentile_{period}'] = df['close'].rolling(period).rank(pct=True)
        
        # 6. é«˜çº§æŠ€æœ¯å› å­
        print("  ğŸ¯ è®¡ç®—é«˜çº§æŠ€æœ¯å› å­...")
        
        # å¸ƒæ—å¸¦
        for period in [20]:
            sma = df['close'].rolling(period).mean()
            std = df['close'].rolling(period).std()
            factors[f'bb_upper_{period}'] = sma + 2 * std
            factors[f'bb_lower_{period}'] = sma - 2 * std
            factors[f'bb_width_{period}'] = (factors[f'bb_upper_{period}'] - factors[f'bb_lower_{period}']) / sma
            factors[f'bb_position_{period}'] = (df['close'] - factors[f'bb_lower_{period}']) / (factors[f'bb_upper_{period}'] - factors[f'bb_lower_{period}'])
        
        # ATR
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift(1))
        low_close = np.abs(df['low'] - df['close'].shift(1))
        true_range = np.maximum(high_low, np.maximum(high_close, low_close))
        factors['atr_14'] = true_range.rolling(14).mean()
        factors['atr_ratio'] = factors['atr_14'] / df['close']
        
        # 7. å¦‚æœæœ‰åŸºæœ¬é¢æ•°æ®ï¼Œæ·»åŠ ä¼°å€¼å› å­
        if data_source == 'tushare' and 'pe_ratio' in df.columns:
            print("  ğŸ’¼ æ·»åŠ åŸºæœ¬é¢å› å­...")
            factors['pe_ratio'] = df['pe_ratio']
            factors['pb_ratio'] = df['pb_ratio'] 
            factors['market_cap_log'] = np.log(df['market_cap'].fillna(df['market_cap'].median()))
            if 'turnover_rate' in df.columns:
                factors['turnover_rate'] = df['turnover_rate']
        
        # 8. è®¡ç®—æœªæ¥æ”¶ç›Šç‡æ ‡ç­¾
        print("  ğŸ¯ è®¡ç®—é¢„æµ‹ç›®æ ‡...")
        for period in [1, 3, 5, 10, 20]:
            factors[f'future_return_{period}d'] = df['close'].shift(-period) / df['close'] - 1
        
        print(f"  âœ… å®Œæˆå› å­è®¡ç®—: {len([k for k in factors.keys() if not k.startswith('future_return')])} ä¸ªå› å­")
        return factors
    
    def analyze_factor_effectiveness_advanced(self, factors, target_period=5):
        """
        é«˜çº§å› å­æœ‰æ•ˆæ€§åˆ†æ
        """
        print(f"ğŸ”¬ è¿›è¡Œé«˜çº§å› å­æœ‰æ•ˆæ€§åˆ†æ (é¢„æµ‹{target_period}æ—¥æ”¶ç›Š)...")
        
        target_col = f'future_return_{target_period}d'
        if target_col not in factors:
            print("âŒ ç¼ºå°‘é¢„æµ‹ç›®æ ‡")
            return {}
        
        future_returns = pd.Series(factors[target_col]).dropna()
        if len(future_returns) < 20:
            print("âŒ æœ‰æ•ˆæ ·æœ¬ä¸è¶³")
            return {}
        
        results = {}
        factor_names = [k for k in factors.keys() if not k.startswith('future_return')]
        
        print(f"  ğŸ“Š åˆ†æ {len(factor_names)} ä¸ªå› å­çš„æœ‰æ•ˆæ€§...")
        
        for factor_name in factor_names:
            factor_values = pd.Series(factors[factor_name])
            
            # å¯¹é½æ•°æ®
            aligned_data = pd.DataFrame({
                'factor': factor_values,
                'return': future_returns
            }).dropna()
            
            if len(aligned_data) < 10:
                continue
            
            factor_vals = aligned_data['factor']
            return_vals = aligned_data['return']
            
            # 1. ç›¸å…³æ€§åˆ†æ
            ic = factor_vals.corr(return_vals)
            rank_ic = factor_vals.rank().corr(return_vals.rank())
            
            # 2. åˆ†ç»„å›æµ‹
            try:
                # åˆ†ä¸º5ç»„
                factor_vals_copy = factor_vals.copy()
                quantiles = pd.qcut(factor_vals_copy, 5, duplicates='drop')
                group_returns = return_vals.groupby(quantiles).agg(['mean', 'std', 'count'])
                
                if len(group_returns) >= 3:
                    # è®¡ç®—å¤šç©ºæ”¶ç›Š
                    long_short_return = group_returns['mean'].iloc[-1] - group_returns['mean'].iloc[0]
                    
                    # è®¡ç®—å•è°ƒæ€§
                    mean_returns = group_returns['mean'].values
                    monotonic_up = sum(mean_returns[i+1] >= mean_returns[i] for i in range(len(mean_returns)-1))
                    monotonic_down = sum(mean_returns[i+1] <= mean_returns[i] for i in range(len(mean_returns)-1))
                    monotonicity = max(monotonic_up, monotonic_down) / (len(mean_returns) - 1)
                    
                    # è®¡ç®—ä¿¡æ¯æ¯”ç‡
                    if group_returns['std'].iloc[-1] > 0 and group_returns['std'].iloc[0] > 0:
                        long_ir = group_returns['mean'].iloc[-1] / group_returns['std'].iloc[-1]
                        short_ir = group_returns['mean'].iloc[0] / group_returns['std'].iloc[0]
                        info_ratio = abs(long_ir) + abs(short_ir)
                    else:
                        info_ratio = 0
                else:
                    long_short_return = 0
                    monotonicity = 0
                    info_ratio = 0
                    
            except:
                long_short_return = 0
                monotonicity = 0
                info_ratio = 0
            
            # ç»¼åˆè¯„åˆ†
            ic_score = abs(ic) if not np.isnan(ic) else 0
            rank_ic_score = abs(rank_ic) if not np.isnan(rank_ic) else 0
            long_short_score = abs(long_short_return) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
            
            final_score = (ic_score * 0.3 + 
                          rank_ic_score * 0.3 + 
                          monotonicity * 0.2 + 
                          long_short_score * 0.2)
            
            results[factor_name] = {
                'ic': ic,
                'rank_ic': rank_ic,
                'long_short_return': long_short_return,
                'monotonicity': monotonicity,
                'info_ratio': info_ratio,
                'sample_size': len(aligned_data),
                'final_score': final_score
            }
        
        print(f"  âœ… å®Œæˆ {len(results)} ä¸ªå› å­çš„æœ‰æ•ˆæ€§åˆ†æ")
        return results
    
    def run_comprehensive_analysis(self, stock_codes, start_date='20250701', end_date='20250731'):
        """
        è¿è¡Œç»¼åˆåˆ†æ
        """
        print("ğŸš€ å¼€å§‹ç»¼åˆè‚¡ç¥¨å› å­åˆ†æ")
        print("=" * 80)
        
        all_results = {}
        
        for i, stock_code in enumerate(stock_codes, 1):
            print(f"\nğŸ“Š [{i}/{len(stock_codes)}] åˆ†æ {stock_code}")
            print("-" * 60)
            
            # è·å–æ•°æ®
            df, data_source = self.get_hybrid_stock_data(stock_code, start_date, end_date)
            
            if df.empty:
                print(f"âŒ {stock_code} æ•°æ®è·å–å¤±è´¥")
                continue
            
            # è®¡ç®—å› å­
            factors = self.calculate_comprehensive_factors(df, data_source)
            
            if not factors:
                print(f"âŒ {stock_code} å› å­è®¡ç®—å¤±è´¥")
                continue
            
            # åˆ†ææœ‰æ•ˆæ€§
            effectiveness = self.analyze_factor_effectiveness_advanced(factors, target_period=5)
            
            if not effectiveness:
                print(f"âŒ {stock_code} å› å­åˆ†æå¤±è´¥")
                continue
            
            # ä¿å­˜ç»“æœ
            all_results[stock_code] = {
                'data_source': data_source,
                'data_records': len(df),
                'factor_count': len([k for k in factors.keys() if not k.startswith('future_return')]),
                'effectiveness': effectiveness
            }
            
            # æ˜¾ç¤ºtopå› å­
            sorted_factors = sorted(effectiveness.items(), key=lambda x: x[1]['final_score'], reverse=True)
            print(f"\\nğŸ† {stock_code} Top 5 å› å­:")
            for j, (factor_name, metrics) in enumerate(sorted_factors[:5], 1):
                print(f"  {j}. {factor_name:<25} | å¾—åˆ†: {metrics['final_score']:.4f} | IC: {metrics['ic']:.4f}")
            
            # é¿å…APIé¢‘ç‡é™åˆ¶
            if data_source == 'tushare' and i < len(stock_codes):
                print("  â³ ç­‰å¾…1ç§’é¿å…APIé™åˆ¶...")
                time.sleep(1)
        
        # æ±‡æ€»åˆ†æ
        if all_results:
            self._generate_summary_report(all_results)
        
        return all_results
    
    def _generate_summary_report(self, all_results):
        """
        ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        """
        print("\\n" + "=" * 80)
        print("ğŸ“‹ ç»¼åˆåˆ†ææ±‡æ€»æŠ¥å‘Š")
        print("=" * 80)
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_stocks = len(all_results)
        tushare_count = sum(1 for r in all_results.values() if r['data_source'] == 'tushare')
        qlib_count = total_stocks - tushare_count
        
        print(f"ğŸ“Š åˆ†ææ¦‚å†µ:")
        print(f"  æ€»è‚¡ç¥¨æ•°: {total_stocks}")
        print(f"  tushareæ•°æ®: {tushare_count} åª")
        print(f"  qlibæ•°æ®: {qlib_count} åª")
        
        # æ”¶é›†æ‰€æœ‰å› å­å¾—åˆ†
        all_factor_scores = {}
        for stock_code, result in all_results.items():
            for factor_name, metrics in result['effectiveness'].items():
                if factor_name not in all_factor_scores:
                    all_factor_scores[factor_name] = []
                all_factor_scores[factor_name].append(metrics['final_score'])
        
        # è®¡ç®—å¹³å‡å¾—åˆ†
        avg_scores = {factor: np.mean(scores) for factor, scores in all_factor_scores.items()}
        top_universal_factors = sorted(avg_scores.items(), key=lambda x: x[1], reverse=True)[:15]
        
        print(f"\\nğŸ† è·¨è‚¡ç¥¨é€šç”¨ä¼˜ç§€å› å­ (Top 15):")
        for i, (factor_name, avg_score) in enumerate(top_universal_factors, 1):
            print(f"  {i:2d}. {factor_name:<30} | å¹³å‡å¾—åˆ†: {avg_score:.4f}")
        
        # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        self._save_comprehensive_report(all_results, top_universal_factors)
    
    def _save_comprehensive_report(self, all_results, top_factors):
        """
        ä¿å­˜è¯¦ç»†æŠ¥å‘Š
        """
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f'tushare_enhanced_analysis_{timestamp}.md'
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("# Tushareå¢å¼ºå› å­ç³»ç»Ÿåˆ†ææŠ¥å‘Š\\n\\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n")
            
            f.write("## åˆ†ææ¦‚å†µ\\n\\n")
            f.write(f"- åˆ†æè‚¡ç¥¨æ•°: {len(all_results)}\\n")
            f.write(f"- æ•°æ®æºåˆ†å¸ƒ: tushare {sum(1 for r in all_results.values() if r['data_source'] == 'tushare')} | qlib {sum(1 for r in all_results.values() if r['data_source'] == 'qlib')}\\n\\n")
            
            f.write("## å„è‚¡ç¥¨åˆ†æç»“æœ\\n\\n")
            for stock_code, result in all_results.items():
                f.write(f"### {stock_code}\\n")
                f.write(f"- æ•°æ®æº: {result['data_source']}\\n")
                f.write(f"- æ•°æ®è®°å½•æ•°: {result['data_records']}\\n")
                f.write(f"- è®¡ç®—å› å­æ•°: {result['factor_count']}\\n\\n")
            
            f.write("## é€šç”¨ä¼˜ç§€å› å­æ’å\\n\\n")
            for i, (factor_name, avg_score) in enumerate(top_factors, 1):
                f.write(f"{i}. **{factor_name}** - å¹³å‡å¾—åˆ†: {avg_score:.4f}\\n")
        
        print(f"\\nğŸ’¾ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """
    ä¸»ç¨‹åº
    """
    print("ğŸŒŸ å¯åŠ¨Tushareå¢å¼ºå› å­ç³»ç»Ÿ")
    
    # åˆå§‹åŒ–ç³»ç»Ÿ
    system = TushareEnhancedFactorSystem('b34d8920b99b43d48df7e792a4708a29f868feeee30d9c84b54bf065')
    
    # æµ‹è¯•è‚¡ç¥¨åˆ—è¡¨
    test_stocks = ['000001.SZ', '000002.SZ', '000006.SZ', '600036.SH', '600000.SH']
    
    # è¿è¡Œç»¼åˆåˆ†æ
    results = system.run_comprehensive_analysis(test_stocks)
    
    print("\\nğŸŠ Tushareå¢å¼ºå› å­ç³»ç»Ÿåˆ†æå®Œæˆï¼")

if __name__ == "__main__":
    main()