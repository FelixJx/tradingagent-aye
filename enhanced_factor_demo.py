# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆå› å­ç³»ç»Ÿæ¼”ç¤º - ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
å±•ç¤º4å±‚å› å­æ¶æ„ + çº¿æ€§/æœºå™¨å­¦ä¹ ç»„åˆå¤„ç†
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple
import warnings
from datetime import datetime, timedelta
warnings.filterwarnings('ignore')

# æœºå™¨å­¦ä¹ å·¥å…·
try:
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.preprocessing import StandardScaler
    from sklearn.metrics import mean_squared_error
    ML_AVAILABLE = True
    print("âœ… æœºå™¨å­¦ä¹ å·¥å…·å¯ç”¨")
except ImportError:
    ML_AVAILABLE = False
    print("âš ï¸ æœºå™¨å­¦ä¹ å·¥å…·ä¸å¯ç”¨")

class EnhancedFactorDemo:
    """
    å¢å¼ºç‰ˆå› å­ç³»ç»Ÿæ¼”ç¤º
    å±•ç¤ºå®Œæ•´çš„4å±‚å› å­æ¶æ„å’Œå¤„ç†æµç¨‹
    """
    
    def __init__(self):
        """åˆå§‹åŒ–æ¼”ç¤ºç³»ç»Ÿ"""
        np.random.seed(42)  # ç¡®ä¿ç»“æœå¯é‡ç°
        
    def generate_mock_stock_data(self, start_date: str, end_date: str, stock_count: int = 1) -> pd.DataFrame:
        """ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®"""
        dates = pd.date_range(start_date, end_date, freq='D')
        # åªä¿ç•™å·¥ä½œæ—¥
        dates = dates[dates.weekday < 5]
        
        print(f"ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®: {len(dates)} ä¸ªäº¤æ˜“æ—¥")
        
        # ç”Ÿæˆä»·æ ¼æ•°æ®ï¼ˆä½¿ç”¨å‡ ä½•å¸ƒæœ—è¿åŠ¨ï¼‰
        returns = np.random.normal(0.001, 0.02, len(dates))  # æ—¥æ”¶ç›Šç‡
        price = 100 * np.exp(np.cumsum(returns))  # ä»·æ ¼åºåˆ—
        
        # ç”ŸæˆOHLCæ•°æ®
        df = pd.DataFrame({
            'trade_date': dates,
            'open': price * (1 + np.random.normal(0, 0.005, len(dates))),
            'high': price * (1 + abs(np.random.normal(0, 0.01, len(dates)))),
            'low': price * (1 - abs(np.random.normal(0, 0.01, len(dates)))),
            'close': price,
            'vol': np.random.lognormal(15, 0.5, len(dates)),  # æˆäº¤é‡
            'turnover_rate': np.random.uniform(0.5, 5.0, len(dates)),  # æ¢æ‰‹ç‡
            'pct_chg': returns * 100  # æ¶¨è·Œå¹…
        })
        
        # ç¡®ä¿OHLCé€»è¾‘æ­£ç¡®
        df['high'] = np.maximum(df['high'], df[['open', 'close']].max(axis=1))
        df['low'] = np.minimum(df['low'], df[['open', 'close']].min(axis=1))
        
        return df
    
    # ==================== Layer 1: åŸºç¡€æŠ€æœ¯å› å­ ====================
    
    def calculate_layer1_basic_factors(self, df: pd.DataFrame) -> Dict:
        """Layer 1: åŸºç¡€æŠ€æœ¯å› å­"""
        factors = {}
        
        print("  - è®¡ç®—ä»·æ ¼åŠ¨é‡å› å­...")
        # ä»·æ ¼å› å­
        for period in [1, 5, 10, 20, 60]:
            factors[f'momentum_{period}'] = (df['close'] - df['close'].shift(period)) / df['close'].shift(period)
            factors[f'reversal_{period}'] = -factors[f'momentum_{period}']  # åè½¬å› å­
        
        print("  - è®¡ç®—æ³¢åŠ¨ç‡å› å­...")
        # æ³¢åŠ¨ç‡å› å­
        returns = df['close'].pct_change()
        for period in [5, 20, 60]:
            factors[f'volatility_{period}'] = returns.rolling(period).std()
            factors[f'vol_ratio_{period}'] = factors[f'volatility_{period}'] / factors[f'volatility_{period}'].rolling(60).mean()
        
        print("  - è®¡ç®—æˆäº¤é‡å› å­...")
        # æˆäº¤é‡å› å­
        for period in [5, 20, 60]:
            factors[f'volume_ratio_{period}'] = df['vol'] / df['vol'].rolling(period).mean()
            factors[f'volume_price_corr_{period}'] = returns.rolling(period).corr(df['vol'].pct_change())
        
        print("  - è®¡ç®—ä»·æ ¼ä½ç½®å› å­...")
        # ä»·æ ¼ä½ç½®å› å­
        for period in [20, 60, 120]:
            high_max = df['high'].rolling(period).max()
            low_min = df['low'].rolling(period).min()
            factors[f'price_position_{period}'] = (df['close'] - low_min) / (high_max - low_min)
        
        print(f"  âœ… Layer 1å®Œæˆ: {len(factors)} ä¸ªåŸºç¡€å› å­")
        return factors
    
    # ==================== Layer 2: é«˜çº§æŠ€æœ¯å› å­ ====================
    
    def calculate_layer2_advanced_factors(self, df: pd.DataFrame) -> Dict:
        """Layer 2: é«˜çº§æŠ€æœ¯å› å­"""
        factors = {}
        
        print("  - è®¡ç®—é«˜çº§è¶‹åŠ¿å› å­...")
        # æ‰‹åŠ¨å®ç°é«˜çº§æŒ‡æ ‡
        
        # ADX (å¹³å‡è¶‹å‘æŒ‡æ ‡)
        high_low = df['high'] - df['low']
        high_close = np.abs(df['high'] - df['close'].shift(1))
        low_close = np.abs(df['low'] - df['close'].shift(1))
        ranges = np.maximum(high_low, np.maximum(high_close, low_close))
        atr = ranges.rolling(14).mean()
        
        plus_dm = np.where((df['high'] - df['high'].shift(1)) > (df['low'].shift(1) - df['low']), 
                          np.maximum(df['high'] - df['high'].shift(1), 0), 0)
        minus_dm = np.where((df['low'].shift(1) - df['low']) > (df['high'] - df['high'].shift(1)), 
                           np.maximum(df['low'].shift(1) - df['low'], 0), 0)
        
        plus_di = 100 * (plus_dm / atr).rolling(14).mean()
        minus_di = 100 * (minus_dm / atr).rolling(14).mean()
        
        factors['adx'] = (100 * np.abs(plus_di - minus_di) / (plus_di + minus_di)).rolling(14).mean()
        factors['atr'] = atr
        factors['atr_ratio'] = atr / df['close']
        
        print("  - è®¡ç®—åŠ¨é‡å¢å¼ºå› å­...")
        # é£é™©è°ƒæ•´åŠ¨é‡å› å­
        returns = df['close'].pct_change()
        for period in [10, 20, 60]:
            momentum = factors.get(f'momentum_{period}', (df['close'] - df['close'].shift(period)) / df['close'].shift(period))
            volatility = returns.rolling(period).std()
            factors[f'risk_adj_momentum_{period}'] = momentum / (volatility + 1e-8)  # é¿å…é™¤é›¶
        
        print("  - è®¡ç®—é«˜é˜¶ç»Ÿè®¡å› å­...")
        # åŠ¨é‡ååº¦å’Œå³°åº¦å› å­
        for period in [20, 60]:
            factors[f'momentum_skew_{period}'] = returns.rolling(period).skew()
            factors[f'momentum_kurt_{period}'] = returns.rolling(period).kurt()
        
        # ä»·æ ¼å†²å‡»å› å­
        factors['price_impact'] = returns / np.log(df['vol'] + 1)
        
        # è®¢å•ä¸å¹³è¡¡ä»£ç†
        factors['order_imbalance_proxy'] = (df['vol'] * np.sign(returns)).cumsum()
        
        print("  - è®¡ç®—å¸‚åœºå¾®è§‚ç»“æ„å› å­...")
        # å¾®è§‚ç»“æ„å› å­
        factors['bid_ask_proxy'] = (df['high'] - df['low']) / df['close']  # ä¹°å–ä»·å·®ä»£ç†
        factors['intraday_momentum'] = (df['close'] - df['open']) / df['open']  # æ—¥å†…åŠ¨é‡
        factors['gap'] = (df['open'] - df['close'].shift(1)) / df['close'].shift(1)  # è·³ç©º
        
        # æˆäº¤é‡ä»·æ ¼è¶‹åŠ¿
        factors['volume_price_trend'] = ((df['close'] - df['close'].shift(1)) / df['close'].shift(1) * df['vol']).cumsum()
        
        print(f"  âœ… Layer 2å®Œæˆ: {len(factors)} ä¸ªé«˜çº§å› å­")
        return factors
    
    # ==================== Layer 3: åŸºæœ¬é¢å› å­ (æ¨¡æ‹Ÿ) ====================
    
    def calculate_layer3_fundamental_factors(self, df: pd.DataFrame) -> Dict:
        """Layer 3: åŸºæœ¬é¢å› å­ (æ¨¡æ‹Ÿæ•°æ®)"""
        factors = {}
        
        print("  - ç”Ÿæˆæ¨¡æ‹ŸåŸºæœ¬é¢æ•°æ®...")
        
        # æ¨¡æ‹Ÿè´¢åŠ¡æŒ‡æ ‡ï¼ˆé€šå¸¸æ˜¯å­£åº¦æ•°æ®ï¼Œè¿™é‡Œç®€åŒ–ä¸ºå¸¸æ•°ï¼‰
        np.random.seed(42)
        
        # ä¼°å€¼å› å­
        factors['pe_ratio'] = 15 + np.random.normal(0, 5)  # å¸‚ç›ˆç‡
        factors['pb_ratio'] = 1.5 + np.random.normal(0, 0.5)  # å¸‚å‡€ç‡
        factors['ps_ratio'] = 2.0 + np.random.normal(0, 0.8)  # å¸‚é”€ç‡
        
        # æˆé•¿å› å­
        factors['revenue_growth'] = 0.15 + np.random.normal(0, 0.1)  # è¥æ”¶å¢é•¿ç‡
        factors['profit_growth'] = 0.12 + np.random.normal(0, 0.15)  # åˆ©æ¶¦å¢é•¿ç‡
        factors['roe_growth'] = 0.08 + np.random.normal(0, 0.08)  # ROEå¢é•¿
        
        # ç›ˆåˆ©èƒ½åŠ›å› å­
        factors['roe'] = 0.12 + np.random.normal(0, 0.05)  # å‡€èµ„äº§æ”¶ç›Šç‡
        factors['roa'] = 0.06 + np.random.normal(0, 0.03)  # æ€»èµ„äº§æ”¶ç›Šç‡
        factors['gross_margin'] = 0.25 + np.random.normal(0, 0.08)  # æ¯›åˆ©ç‡
        factors['net_margin'] = 0.08 + np.random.normal(0, 0.04)  # å‡€åˆ©ç‡
        
        # è´¢åŠ¡å¥åº·å› å­
        factors['debt_ratio'] = 0.4 + np.random.normal(0, 0.15)  # èµ„äº§è´Ÿå€ºç‡
        factors['current_ratio'] = 1.5 + np.random.normal(0, 0.3)  # æµåŠ¨æ¯”ç‡
        factors['quick_ratio'] = 1.2 + np.random.normal(0, 0.25)  # é€ŸåŠ¨æ¯”ç‡
        
        # ç°é‡‘æµå› å­
        factors['ocf_to_revenue'] = 0.12 + np.random.normal(0, 0.05)  # ç»è¥ç°é‡‘æµ/è¥æ”¶
        factors['fcf_yield'] = 0.05 + np.random.normal(0, 0.03)  # è‡ªç”±ç°é‡‘æµæ”¶ç›Šç‡
        
        # åŸºæœ¬é¢è´¨é‡ç»¼åˆè¯„åˆ†
        factors['fundamental_quality'] = (
            (factors['roe'] - 0.1) * 2 +  # ROEæƒé‡
            (factors['revenue_growth'] - 0.1) * 1.5 +  # æˆé•¿æ€§æƒé‡  
            (0.5 - factors['debt_ratio']) * 1 +  # è´¢åŠ¡å®‰å…¨æƒé‡
            factors['ocf_to_revenue'] * 3  # ç°é‡‘æµæƒé‡
        )
        
        print(f"  âœ… Layer 3å®Œæˆ: {len(factors)} ä¸ªåŸºæœ¬é¢å› å­")
        return factors
    
    # ==================== Layer 4: å¦ç±»å› å­ (æ¨¡æ‹Ÿ) ====================
    
    def calculate_layer4_alternative_factors(self, df: pd.DataFrame) -> Dict:
        """Layer 4: å¦ç±»å› å­ (æ¨¡æ‹Ÿæ•°æ®)"""
        factors = {}
        
        print("  - ç”Ÿæˆæ¨¡æ‹Ÿå¦ç±»æ•°æ®...")
        
        # èµ„é‡‘æµå‘å› å­
        factors['main_net_inflow'] = np.random.normal(1000000, 5000000)  # ä¸»åŠ›å‡€æµå…¥
        factors['main_net_inflow_rate'] = np.random.normal(0.02, 0.05)  # ä¸»åŠ›å‡€æµå…¥ç‡
        factors['northbound_holding'] = max(0, np.random.normal(50000000, 20000000))  # åŒ—å‘èµ„é‡‘æŒè‚¡
        
        # èèµ„èåˆ¸å› å­
        factors['margin_balance'] = max(0, np.random.normal(100000000, 50000000))  # èèµ„ä½™é¢
        factors['margin_buy_ratio'] = np.random.uniform(0.05, 0.3)  # èèµ„ä¹°å…¥å æ¯”
        factors['margin_trend'] = np.random.normal(0.01, 0.1)  # èèµ„ä½™é¢å˜åŒ–è¶‹åŠ¿
        
        # æƒ…ç»ªå› å­
        factors['turnover_anomaly'] = df['turnover_rate'].iloc[-20:].mean() / df['turnover_rate'].mean()  # æ¢æ‰‹ç‡å¼‚å¸¸
        factors['volume_spike'] = df['vol'].iloc[-5:].mean() / df['vol'].iloc[-20:].mean()  # æˆäº¤é‡æ”¾å¤§
        
        # æŠ€æœ¯é¢æƒ…ç»ª
        factors['limit_up_freq'] = sum(df['pct_chg'].tail(60) >= 9.8) / 60  # æ¶¨åœé¢‘ç‡
        factors['limit_down_freq'] = sum(df['pct_chg'].tail(60) <= -9.8) / 60  # è·Œåœé¢‘ç‡
        
        # ç›¸å¯¹å¼ºåº¦å› å­
        market_return = np.random.normal(0.001, 0.015, len(df))  # æ¨¡æ‹Ÿå¸‚åœºæ”¶ç›Š
        stock_return = df['close'].pct_change()
        factors['relative_strength_60'] = (stock_return.tail(60).mean() - np.mean(market_return[-60:]))  # ç›¸å¯¹å¼ºåº¦
        
        # è¡Œä¸šè½®åŠ¨å› å­
        factors['industry_momentum'] = np.random.normal(0.02, 0.08)  # è¡Œä¸šåŠ¨é‡
        factors['industry_relative_pe'] = np.random.normal(1.0, 0.3)  # è¡Œä¸šç›¸å¯¹ä¼°å€¼
        
        # å®è§‚æ•æ„Ÿæ€§å› å­
        factors['macro_beta'] = np.random.normal(1.0, 0.4)  # å®è§‚æ•æ„Ÿåº¦
        factors['policy_sensitivity'] = np.random.uniform(0.1, 2.0)  # æ”¿ç­–æ•æ„Ÿåº¦
        
        # å¦ç±»æ•°æ®ç»¼åˆå¾—åˆ†
        factors['alternative_composite'] = (
            factors['main_net_inflow_rate'] * 2 +
            factors['relative_strength_60'] * 3 +
            factors['industry_momentum'] * 1.5 +
            (factors['turnover_anomaly'] - 1) * 1
        )
        
        print(f"  âœ… Layer 4å®Œæˆ: {len(factors)} ä¸ªå¦ç±»å› å­")
        return factors
    
    # ==================== å› å­å¤„ç†ï¼šçº¿æ€§ vs æœºå™¨å­¦ä¹  ====================
    
    def process_factors_hybrid_approach(self, factor_df: pd.DataFrame, future_returns: pd.Series) -> Dict:
        """æ··åˆå¤„ç†æ–¹æ³•ï¼šçº¿æ€§ç­›é€‰ + æœºå™¨å­¦ä¹ å¢å¼º"""
        
        print("\nå¼€å§‹å› å­å¤„ç†åˆ†æ...")
        
        # ç¬¬1å±‚ï¼šçº¿æ€§å¿«é€Ÿç­›é€‰
        print("  Step 1: çº¿æ€§å…³ç³»ç­›é€‰...")
        linear_results = self.linear_factor_screening(factor_df, future_returns)
        
        # ç¬¬2å±‚ï¼šæœºå™¨å­¦ä¹ æ·±åº¦åˆ†æ
        print("  Step 2: æœºå™¨å­¦ä¹ æ·±åº¦åˆ†æ...")
        if ML_AVAILABLE:
            ml_results = self.ml_factor_analysis(factor_df, future_returns)
        else:
            ml_results = {}
        
        # ç¬¬3å±‚ï¼šé›†æˆè¯„åˆ†
        print("  Step 3: é›†æˆè¯„åˆ†ä¼˜åŒ–...")
        final_results = self.ensemble_factor_scoring(linear_results, ml_results)
        
        # ç¬¬4å±‚ï¼šå› å­é€‰æ‹©
        print("  Step 4: æœ€ä¼˜å› å­ç»„åˆé€‰æ‹©...")
        selected_factors = self.greedy_factor_selection(final_results, factor_df)
        
        return {
            'linear_results': linear_results,
            'ml_results': ml_results,
            'final_scores': final_results,
            'selected_factors': selected_factors,
            'summary': self.generate_analysis_summary(linear_results, ml_results, final_results)
        }
    
    def linear_factor_screening(self, factor_df: pd.DataFrame, future_returns: pd.Series) -> Dict:
        """çº¿æ€§å…³ç³»ç­›é€‰"""
        results = {}
        
        for factor_name in factor_df.columns:
            factor_values = factor_df[factor_name].dropna()
            if len(factor_values) < 30:  # æ•°æ®ä¸è¶³
                continue
                
            aligned_returns = future_returns.loc[factor_values.index]
            
            # IC (ä¿¡æ¯ç³»æ•°)
            ic = factor_values.corr(aligned_returns)
            
            # Rank IC
            rank_ic = factor_values.rank().corr(aligned_returns.rank())
            
            # åˆ†ç»„å•è°ƒæ€§æµ‹è¯•
            monotonicity = self.calculate_monotonicity(factor_values, aligned_returns)
            
            # ç¨³å®šæ€§æµ‹è¯•
            stability = self.calculate_ic_stability(factor_values, aligned_returns)
            
            results[factor_name] = {
                'ic': ic if not np.isnan(ic) else 0,
                'rank_ic': rank_ic if not np.isnan(rank_ic) else 0,
                'monotonicity': monotonicity,
                'stability': stability,
                'linear_score': self.calculate_linear_composite_score(ic, rank_ic, monotonicity, stability)
            }
        
        return results
    
    def ml_factor_analysis(self, factor_df: pd.DataFrame, future_returns: pd.Series) -> Dict:
        """æœºå™¨å­¦ä¹ å› å­åˆ†æ"""
        results = {}
        
        try:
            # æ•°æ®æ¸…æ´—
            clean_df = factor_df.fillna(method='ffill').fillna(0)
            aligned_returns = future_returns.loc[clean_df.index]
            
            if len(clean_df) < 50:
                return results
            
            # ç‰¹å¾é‡è¦æ€§åˆ†æ
            rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)
            rf.fit(clean_df, aligned_returns)
            
            feature_importance = rf.feature_importances_
            model_score = rf.score(clean_df, aligned_returns)
            
            # å•å› å­éçº¿æ€§åˆ†æ
            for i, factor_name in enumerate(clean_df.columns):
                factor_data = clean_df[[factor_name]]
                
                # å•å› å­éšæœºæ£®æ—
                single_rf = RandomForestRegressor(n_estimators=50, random_state=42)
                single_rf.fit(factor_data, aligned_returns)
                single_score = single_rf.score(factor_data, aligned_returns)
                
                # éçº¿æ€§å…³ç³»æ£€æµ‹
                nonlinear_score = self.detect_nonlinear_relationship(clean_df[factor_name], aligned_returns)
                
                results[factor_name] = {
                    'feature_importance': feature_importance[i],
                    'single_factor_r2': single_score,
                    'nonlinear_score': nonlinear_score,
                    'ml_composite_score': (feature_importance[i] * 0.5 + 
                                         single_score * 0.3 + 
                                         nonlinear_score * 0.2)
                }
            
            # äº¤äº’é¡¹åˆ†æ (é€‰æ‹©top 10å› å­)
            top_factors = sorted(results.items(), key=lambda x: x[1]['feature_importance'], reverse=True)[:10]
            
            for i, (factor1, _) in enumerate(top_factors[:5]):  # é™åˆ¶äº¤äº’é¡¹æ•°é‡
                for j, (factor2, _) in enumerate(top_factors[i+1:6], i+1):
                    interaction_name = f"{factor1}_x_{factor2}"
                    interaction_values = clean_df[factor1] * clean_df[factor2]
                    
                    # äº¤äº’é¡¹é¢„æµ‹èƒ½åŠ›
                    interaction_rf = RandomForestRegressor(n_estimators=30, random_state=42)
                    interaction_rf.fit(interaction_values.values.reshape(-1, 1), aligned_returns)
                    interaction_score = interaction_rf.score(interaction_values.values.reshape(-1, 1), aligned_returns)
                    
                    results[interaction_name] = {
                        'feature_importance': 0,  # äº¤äº’é¡¹æ²¡æœ‰ç›´æ¥é‡è¦æ€§
                        'single_factor_r2': interaction_score,
                        'nonlinear_score': interaction_score,
                        'ml_composite_score': interaction_score * 0.8,  # äº¤äº’é¡¹æƒé‡ç¨ä½
                        'is_interaction': True
                    }
            
        except Exception as e:
            print(f"    æœºå™¨å­¦ä¹ åˆ†æå¤±è´¥: {e}")
        
        return results
    
    def detect_nonlinear_relationship(self, factor_values: pd.Series, returns: pd.Series) -> float:
        """æ£€æµ‹éçº¿æ€§å…³ç³»"""
        try:
            # ä½¿ç”¨å¤šé¡¹å¼ç‰¹å¾æ£€æµ‹éçº¿æ€§å…³ç³»
            from sklearn.preprocessing import PolynomialFeatures
            from sklearn.linear_model import LinearRegression
            
            # çº¿æ€§å…³ç³»
            linear_r2 = LinearRegression().fit(factor_values.values.reshape(-1, 1), returns).score(
                factor_values.values.reshape(-1, 1), returns)
            
            # äºŒæ¬¡å…³ç³»
            poly_features = PolynomialFeatures(degree=2)
            factor_poly = poly_features.fit_transform(factor_values.values.reshape(-1, 1))
            poly_r2 = LinearRegression().fit(factor_poly, returns).score(factor_poly, returns)
            
            # éçº¿æ€§æå‡åº¦
            nonlinear_improvement = max(0, poly_r2 - linear_r2)
            
            return min(nonlinear_improvement * 5, 1.0)  # å½’ä¸€åŒ–åˆ°[0,1]
            
        except:
            return 0.0
    
    def calculate_monotonicity(self, factor_values: pd.Series, returns: pd.Series) -> float:
        """è®¡ç®—å› å­å•è°ƒæ€§"""
        try:
            # åˆ†ä¸º5ä¸ªåˆ†ä½æ•°ç»„
            quantiles = pd.qcut(factor_values, 5, duplicates='drop')
            group_returns = returns.groupby(quantiles).mean()
            
            if len(group_returns) < 3:
                return 0
            
            # è®¡ç®—å•è°ƒæ€§
            monotonic_increases = sum(group_returns.iloc[i+1] >= group_returns.iloc[i] 
                                    for i in range(len(group_returns)-1))
            monotonic_decreases = sum(group_returns.iloc[i+1] <= group_returns.iloc[i] 
                                    for i in range(len(group_returns)-1))
            
            total_comparisons = len(group_returns) - 1
            return max(monotonic_increases, monotonic_decreases) / total_comparisons
            
        except:
            return 0
    
    def calculate_ic_stability(self, factor_values: pd.Series, returns: pd.Series, window: int = 60) -> float:
        """è®¡ç®—ICç¨³å®šæ€§"""
        try:
            if len(factor_values) < window * 2:
                return 0
            
            rolling_ics = []
            for i in range(window, len(factor_values)):
                window_factor = factor_values.iloc[i-window:i]
                window_returns = returns.iloc[i-window:i]
                ic = window_factor.corr(window_returns)
                if not np.isnan(ic):
                    rolling_ics.append(ic)
            
            if len(rolling_ics) < 5:
                return 0
            
            # ICç¨³å®šæ€§ = 1 - ICæ ‡å‡†å·®/ICå‡å€¼çš„ç»å¯¹å€¼
            ic_mean = np.mean(rolling_ics)
            ic_std = np.std(rolling_ics)
            
            if abs(ic_mean) > 0:
                stability = 1 - min(ic_std / abs(ic_mean), 2)  # é™åˆ¶åœ¨åˆç†èŒƒå›´
                return max(stability, 0)
            else:
                return 0
                
        except:
            return 0
    
    def calculate_linear_composite_score(self, ic: float, rank_ic: float, monotonicity: float, stability: float) -> float:
        """è®¡ç®—çº¿æ€§ç»¼åˆå¾—åˆ†"""
        # æƒé‡é…ç½®
        weights = {
            'ic': 0.4,
            'rank_ic': 0.3,
            'monotonicity': 0.2,
            'stability': 0.1
        }
        
        # å¤„ç†NaNå€¼
        ic = ic if not np.isnan(ic) else 0
        rank_ic = rank_ic if not np.isnan(rank_ic) else 0
        
        score = (abs(ic) * weights['ic'] + 
                abs(rank_ic) * weights['rank_ic'] +
                monotonicity * weights['monotonicity'] +
                stability * weights['stability'])
        
        return score
    
    def ensemble_factor_scoring(self, linear_results: Dict, ml_results: Dict) -> Dict:
        """é›†æˆå› å­è¯„åˆ†"""
        final_scores = {}
        
        all_factors = set(linear_results.keys()) | set(ml_results.keys())
        
        for factor in all_factors:
            linear_score = linear_results.get(factor, {}).get('linear_score', 0)
            ml_score = ml_results.get(factor, {}).get('ml_composite_score', 0)
            
            # åŠ¨æ€æƒé‡ï¼šå¦‚æœMLåˆ†æå¯ç”¨ä¸”æ•ˆæœå¥½ï¼Œå¢åŠ MLæƒé‡
            if ML_AVAILABLE and ml_score > 0.1:
                ml_weight = 0.6
                linear_weight = 0.4
            else:
                ml_weight = 0.2
                linear_weight = 0.8
            
            final_score = linear_weight * linear_score + ml_weight * ml_score
            
            final_scores[factor] = {
                'linear_component': linear_score,
                'ml_component': ml_score,
                'final_score': final_score,
                'linear_weight': linear_weight,
                'ml_weight': ml_weight
            }
        
        return final_scores
    
    def greedy_factor_selection(self, final_scores: Dict, factor_df: pd.DataFrame, max_factors: int = 12) -> List[str]:
        """è´ªå¿ƒç®—æ³•é€‰æ‹©æœ€ä¼˜å› å­ç»„åˆ"""
         
        # æŒ‰å¾—åˆ†æ’åº
        sorted_factors = sorted(final_scores.items(), key=lambda x: x[1]['final_score'], reverse=True)
        
        selected = []
        correlation_matrix = factor_df.corr()
        
        for factor_name, scores in sorted_factors:
            if len(selected) >= max_factors:
                break
            
            if factor_name not in correlation_matrix.columns:
                continue
            
            # æ£€æŸ¥ä¸å·²é€‰å› å­çš„ç›¸å…³æ€§
            max_correlation = 0
            if selected:
                correlations = [abs(correlation_matrix.loc[factor_name, selected_factor]) 
                              for selected_factor in selected 
                              if selected_factor in correlation_matrix.columns]
                max_correlation = max(correlations) if correlations else 0
            
            # ç›¸å…³æ€§é˜ˆå€¼ï¼šé¿å…é€‰æ‹©é«˜åº¦ç›¸å…³çš„å› å­
            if max_correlation < 0.7:  # ç›¸å…³æ€§é˜ˆå€¼
                selected.append(factor_name)
        
        return selected
    
    def generate_analysis_summary(self, linear_results: Dict, ml_results: Dict, final_results: Dict) -> Dict:
        """ç”Ÿæˆåˆ†ææ€»ç»“"""
        summary = {
            'total_factors_analyzed': len(final_results),
            'linear_analysis': {
                'factors_count': len(linear_results),
                'avg_ic': np.mean([r['ic'] for r in linear_results.values()]),
                'avg_rank_ic': np.mean([r['rank_ic'] for r in linear_results.values()]),
                'avg_monotonicity': np.mean([r['monotonicity'] for r in linear_results.values()]),
                'avg_stability': np.mean([r['stability'] for r in linear_results.values()])
            },
            'ml_analysis': {
                'factors_count': len(ml_results),
                'avg_feature_importance': np.mean([r['feature_importance'] for r in ml_results.values()]) if ml_results else 0,
                'avg_r2_score': np.mean([r['single_factor_r2'] for r in ml_results.values()]) if ml_results else 0,
                'interaction_factors': sum(1 for r in ml_results.values() if r.get('is_interaction', False))
            },
            'final_ranking': {
                'top_10_factors': sorted(final_results.items(), key=lambda x: x[1]['final_score'], reverse=True)[:10]
            }
        }
        
        return summary
    
    # ==================== ä¸»è¦æ¼”ç¤ºæ¥å£ ====================
    
    def run_complete_demo(self):
        """è¿è¡Œå®Œæ•´æ¼”ç¤º"""
        print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆå› å­ç³»ç»Ÿå®Œæ•´æ¼”ç¤º")
        print("=" * 60)
        
        # 1. ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        print("\nğŸ“Š Step 1: ç”Ÿæˆæ¨¡æ‹Ÿè‚¡ç¥¨æ•°æ®")
        df = self.generate_mock_stock_data('2023-01-01', '2024-01-31')
        
        # 2. 4å±‚å› å­è®¡ç®—
        print("\nğŸ§® Step 2: è®¡ç®—4å±‚å› å­æ¶æ„")
        
        # Layer 1
        print("\nLayer 1: åŸºç¡€æŠ€æœ¯å› å­")
        layer1_factors = self.calculate_layer1_basic_factors(df)
        
        # Layer 2  
        print("\nLayer 2: é«˜çº§æŠ€æœ¯å› å­")
        layer2_factors = self.calculate_layer2_advanced_factors(df)
        
        # Layer 3
        print("\nLayer 3: åŸºæœ¬é¢å› å­")
        layer3_factors = self.calculate_layer3_fundamental_factors(df)
        
        # Layer 4
        print("\nLayer 4: å¦ç±»å› å­")
        layer4_factors = self.calculate_layer4_alternative_factors(df)
        
        # 3. åˆå¹¶å› å­çŸ©é˜µ
        print("\nğŸ”— Step 3: æ„å»ºå®Œæ•´å› å­çŸ©é˜µ")
        all_factors = {}
        all_factors.update(layer1_factors)
        all_factors.update(layer2_factors)
        
        # å¹¿æ’­åŸºæœ¬é¢å’Œå¦ç±»å› å­åˆ°æ—¶é—´åºåˆ—
        for factor_name, factor_value in layer3_factors.items():
            all_factors[f"fundamental_{factor_name}"] = [factor_value] * len(df)
        
        for factor_name, factor_value in layer4_factors.items():
            all_factors[f"alternative_{factor_name}"] = [factor_value] * len(df)
        
        factor_df = pd.DataFrame(all_factors, index=df.index)
        factor_df['future_return_20d'] = df['close'].shift(-20) / df['close'] - 1
        
        print(f"  âœ… å› å­çŸ©é˜µæ„å»ºå®Œæˆ: {factor_df.shape[1]-1} ä¸ªå› å­")
        
        # 4. å› å­åˆ†æå¤„ç†
        print("\nğŸ” Step 4: å› å­æœ‰æ•ˆæ€§åˆ†æ")
        future_returns = factor_df['future_return_20d'].dropna()
        factor_data = factor_df.drop('future_return_20d', axis=1).loc[future_returns.index]
        
        analysis_results = self.process_factors_hybrid_approach(factor_data, future_returns)
        
        # 5. ç»“æœå±•ç¤º
        print("\nğŸ“ˆ Step 5: åˆ†æç»“æœå±•ç¤º")
        self.display_results(analysis_results, factor_df)
        
        # 6. ä¿å­˜ç»“æœ
        print("\nğŸ’¾ Step 6: ä¿å­˜åˆ†æç»“æœ")
        self.save_demo_results(factor_df, analysis_results)
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        return factor_df, analysis_results
    
    def display_results(self, analysis_results: Dict, factor_df: pd.DataFrame):
        """å±•ç¤ºåˆ†æç»“æœ"""
        
        print("\n" + "="*60)
        print("ğŸ“Š å› å­åˆ†æç»“æœæ€»ç»“")
        print("="*60)
        
        summary = analysis_results['summary']
        
        print(f"\nğŸ”¢ åˆ†ææ¦‚å†µ:")
        print(f"  - æ€»å› å­æ•°é‡: {summary['total_factors_analyzed']}")
        print(f"  - çº¿æ€§åˆ†æå› å­: {summary['linear_analysis']['factors_count']}")
        print(f"  - æœºå™¨å­¦ä¹ åˆ†æå› å­: {summary['ml_analysis']['factors_count']}")
        print(f"  - äº¤äº’é¡¹å› å­: {summary['ml_analysis']['interaction_factors']}")
        
        print(f"\nğŸ“ çº¿æ€§åˆ†æè¡¨ç°:")
        print(f"  - å¹³å‡IC: {summary['linear_analysis']['avg_ic']:.4f}")
        print(f"  - å¹³å‡Rank IC: {summary['linear_analysis']['avg_rank_ic']:.4f}")
        print(f"  - å¹³å‡å•è°ƒæ€§: {summary['linear_analysis']['avg_monotonicity']:.4f}")
        print(f"  - å¹³å‡ç¨³å®šæ€§: {summary['linear_analysis']['avg_stability']:.4f}")
        
        if ML_AVAILABLE:
            print(f"\nğŸ¤– æœºå™¨å­¦ä¹ è¡¨ç°:")
            print(f"  - å¹³å‡ç‰¹å¾é‡è¦æ€§: {summary['ml_analysis']['avg_feature_importance']:.4f}")
            print(f"  - å¹³å‡RÂ²å¾—åˆ†: {summary['ml_analysis']['avg_r2_score']:.4f}")
        
        print(f"\nğŸ† Top 10 æœ€ä½³å› å­:")
        for i, (factor_name, scores) in enumerate(summary['final_ranking']['top_10_factors'], 1):
            print(f"  {i:2d}. {factor_name:<25} | å¾—åˆ†: {scores['final_score']:.4f} "
                  f"| çº¿æ€§: {scores['linear_component']:.4f} | ML: {scores['ml_component']:.4f}")
        
        print(f"\nâœ… æœ€ç»ˆé€‰æ‹©çš„å› å­ç»„åˆ ({len(analysis_results['selected_factors'])}ä¸ª):")
        for i, factor in enumerate(analysis_results['selected_factors'], 1):
            score = analysis_results['final_scores'][factor]['final_score']
            print(f"  {i:2d}. {factor:<25} | å¾—åˆ†: {score:.4f}")
    
    def save_demo_results(self, factor_df: pd.DataFrame, analysis_results: Dict):
        """ä¿å­˜æ¼”ç¤ºç»“æœ"""
        
        # ä¿å­˜å› å­çŸ©é˜µ
        factor_df_clean = factor_df.fillna(0)
        factor_df_clean.to_csv('enhanced_factor_demo_matrix.csv', index=False)
        
        # ä¿å­˜åˆ†æç»“æœ
        results_summary = {
            'analysis_summary': analysis_results['summary'],
            'selected_factors': analysis_results['selected_factors'],
            'top_10_scores': {
                factor: scores for factor, scores in 
                sorted(analysis_results['final_scores'].items(), 
                       key=lambda x: x[1]['final_score'], reverse=True)[:10]
            }
        }
        
        import json
        with open('enhanced_factor_demo_results.json', 'w', encoding='utf-8') as f:
            json.dump(results_summary, f, ensure_ascii=False, indent=2)
        
        print("  âœ… å› å­çŸ©é˜µå·²ä¿å­˜: enhanced_factor_demo_matrix.csv")
        print("  âœ… åˆ†æç»“æœå·²ä¿å­˜: enhanced_factor_demo_results.json")

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    demo = EnhancedFactorDemo()
    factor_df, analysis_results = demo.run_complete_demo()
    
    print(f"\nğŸ’¡ ç³»ç»Ÿèƒ½åŠ›å±•ç¤º:")
    print(f"  âœ… 4å±‚å› å­æ¶æ„: Layer1(åŸºç¡€) + Layer2(é«˜çº§) + Layer3(åŸºæœ¬é¢) + Layer4(å¦ç±»)")
    print(f"  âœ… æ··åˆå¤„ç†æ–¹æ³•: çº¿æ€§ç­›é€‰ + æœºå™¨å­¦ä¹ å¢å¼º + é›†æˆè¯„åˆ†")
    print(f"  âœ… æ™ºèƒ½å› å­é€‰æ‹©: è´ªå¿ƒç®—æ³• + ç›¸å…³æ€§è¿‡æ»¤")
    print(f"  âœ… å®Œæ•´åˆ†ææŠ¥å‘Š: çº¿æ€§/éçº¿æ€§/ç¨³å®šæ€§/å•è°ƒæ€§å…¨é¢è¯„ä¼°")
    
    print(f"\nğŸ¯ ç›¸æ¯”åŸæœ‰Agentç³»ç»Ÿçš„æå‡:")
    print(f"  ğŸ“ˆ å› å­æ•°é‡: 20ä¸ªåŸºç¡€æŒ‡æ ‡ â†’ {factor_df.shape[1]-1}ä¸ªé«˜çº§å› å­ (æå‡{((factor_df.shape[1]-1)/20-1)*100:.0f}%)")
    print(f"  ğŸ” åˆ†æç»´åº¦: å•ä¸€æŠ€æœ¯åˆ†æ â†’ 4å±‚å¤šç»´åº¦åˆ†æ")
    print(f"  ğŸ¤– å¤„ç†æ–¹æ³•: çº¯çº¿æ€§é€»è¾‘ â†’ çº¿æ€§+æœºå™¨å­¦ä¹ æ··åˆ")
    print(f"  ğŸ“Š è¯„ä¼°æ ‡å‡†: ç®€å•å‡†ç¡®ç‡ â†’ IC/å•è°ƒæ€§/ç¨³å®šæ€§ç»¼åˆè¯„ä¼°")

if __name__ == "__main__":
    main()